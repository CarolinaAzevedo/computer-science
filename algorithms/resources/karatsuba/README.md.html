<!DOCTYPE html>
<!-- saved from url=(0077)http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <title>README.md</title>
  <style>.markdown-body {margin: 2em auto; padding: 1em; max-width: 800px;}</style>
  <style>
/**
 *  https://github.com/sindresorhus/github-markdown-css
 */

@font-face {
  font-family: octicons-anchor;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

.markdown-body {
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #333;
  overflow: hidden;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body strong {
  font-weight: bold;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body * {
  box-sizing: border-box;
}

.markdown-body input {
  font: 13px/1.4 Helvetica, arial, nimbussansl, liberationsans, freesans, clean, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
}

.markdown-body a {
  color: #4183c4;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before {
  display: table;
  content: "";
}

.markdown-body hr:after {
  display: table;
  clear: both;
  content: "";
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
  font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body .octicon {
  font: normal normal normal 16px/1 octicons-anchor;
  display: inline-block;
  text-decoration: none;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.markdown-body .octicon-link:before {
  content: '\f05c';
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body a:not([href]) {
  color: inherit;
  text-decoration: none;
}

.markdown-body .anchor {
  position: absolute;
  top: 0;
  left: 0;
  display: block;
  padding-right: 6px;
  padding-left: 30px;
  margin-left: -30px;
}

.markdown-body .anchor:focus {
  outline: none;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  position: relative;
  margin-top: 1em;
  margin-bottom: 16px;
  font-weight: bold;
  line-height: 1.4;
}

.markdown-body h1 .octicon-link,
.markdown-body h2 .octicon-link,
.markdown-body h3 .octicon-link,
.markdown-body h4 .octicon-link,
.markdown-body h5 .octicon-link,
.markdown-body h6 .octicon-link {
  display: none;
  color: #000;
  vertical-align: middle;
}

.markdown-body h1:hover .anchor,
.markdown-body h2:hover .anchor,
.markdown-body h3:hover .anchor,
.markdown-body h4:hover .anchor,
.markdown-body h5:hover .anchor,
.markdown-body h6:hover .anchor {
  padding-left: 8px;
  margin-left: -30px;
  text-decoration: none;
}

.markdown-body h1:hover .anchor .octicon-link,
.markdown-body h2:hover .anchor .octicon-link,
.markdown-body h3:hover .anchor .octicon-link,
.markdown-body h4:hover .anchor .octicon-link,
.markdown-body h5:hover .anchor .octicon-link,
.markdown-body h6:hover .anchor .octicon-link {
  display: inline-block;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h1 .anchor {
  line-height: 1;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 .anchor {
  line-height: 1;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h3 .anchor {
  line-height: 1.2;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h4 .anchor {
  line-height: 1.2;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h5 .anchor {
  line-height: 1.1;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body h6 .anchor {
  line-height: 1.1;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  box-sizing: border-box;
}

.markdown-body code {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .highlight pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

.markdown-body kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb;
}

.markdown-body .pl-c {
  color: #969896;
}

.markdown-body .pl-c1,
.markdown-body .pl-s .pl-v {
  color: #0086b3;
}

.markdown-body .pl-e,
.markdown-body .pl-en {
  color: #795da3;
}

.markdown-body .pl-s .pl-s1,
.markdown-body .pl-smi {
  color: #333;
}

.markdown-body .pl-ent {
  color: #63a35c;
}

.markdown-body .pl-k {
  color: #a71d5d;
}

.markdown-body .pl-pds,
.markdown-body .pl-s,
.markdown-body .pl-s .pl-pse .pl-s1,
.markdown-body .pl-sr,
.markdown-body .pl-sr .pl-cce,
.markdown-body .pl-sr .pl-sra,
.markdown-body .pl-sr .pl-sre {
  color: #183691;
}

.markdown-body .pl-v {
  color: #ed6a43;
}

.markdown-body .pl-id {
  color: #b52a1d;
}

.markdown-body .pl-ii {
  background-color: #b52a1d;
  color: #f8f8f8;
}

.markdown-body .pl-sr .pl-cce {
  color: #63a35c;
  font-weight: bold;
}

.markdown-body .pl-ml {
  color: #693a17;
}

.markdown-body .pl-mh,
.markdown-body .pl-mh .pl-en,
.markdown-body .pl-ms {
  color: #1d3e81;
  font-weight: bold;
}

.markdown-body .pl-mq {
  color: #008080;
}

.markdown-body .pl-mi {
  color: #333;
  font-style: italic;
}

.markdown-body .pl-mb {
  color: #333;
  font-weight: bold;
}

.markdown-body .pl-md {
  background-color: #ffecec;
  color: #bd2c00;
}

.markdown-body .pl-mi1 {
  background-color: #eaffea;
  color: #55a532;
}

.markdown-body .pl-mdr {
  color: #795da3;
  font-weight: bold;
}

.markdown-body .pl-mo {
  color: #1d3e81;
}

.markdown-body kbd {
  display: inline-block;
  padding: 3px 5px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb;
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 0.35em 0.25em -1.6em;
  vertical-align: middle;
}

.markdown-body :checked+.radio-label {
  z-index: 1;
  position: relative;
  border-color: #4183c4;
}
</style>
  <link rel="stylesheet" href="./README.md_files/github.css">
  <script src="./README.md_files/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <style>.MathJax:focus, body :focus .MathJax { outline: none; }</style>
  <script type="text/x-mathjax-config;executed=true">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX","output/HTML-CSS"],
      "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: [ "TeX" ],
        linebreaks: { automatic: true },
        EqnChunk: 10,
        imageFont: null
      },
      tex2jax: {
        inlineMath: [["$","$"],["\\(","\\)"],["\\\\(","\\\\)"]],
        displayMath: [["$$","$$"],["\\[","\\]"]],
        processEscapes: false
      },
      messageStyle: "none"
    });
  </script>
  <script src="./README.md_files/MathJax.js"></script><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style>
<style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style></head>
<body><div id="MathJax_Message" style="display: none;"></div>
  <div class="markdown-body">
    <h1 id="computerscience">Computer Science</h1>
<p>This page contains <em>"under the hood"</em>  explanations of theoretical concepts of subjects like <strong>Algorithms</strong>, <strong>Data Structures</strong>, <strong>UNIX Operating Systems</strong> and <strong>Computer Networks</strong> (etc…). Several <a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/coding-problems/">coding problems</a> were also studied and solved, always including a time and memory complexity analysis. 
<br></p>
<div>
<p align="center">
  <i>For those who want to have a better understanding of computer science</i>
</p>
</div>
<h2 id="index">Index</h2>
<ol>
<li>Computer Architecture</li>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#algorithms-and-data-structures">Algorithms and Data Structures</a><ol>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#data-structures">Data Structures</a></li>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#algorithms">Algorithms</a></li></ol></li>
<li>Data Bases</li>
<li>Operating Systems</li>
<li>Computer Networks</li>
<li>Parallel Computing</li>
<li>Distributed Systems</li>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#programming-languages">Programming Languages</a><ol>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#java">Java</a></li>
<li>SQL</li></ol></li>
</ol>
<h2 id="algorithmsanddatastructures">Algorithms and Data Structures</h2>
<p>An <strong>algorithm</strong> is a sequence of computational steps that transform or analyse the input and generates an output. A <strong>data structure</strong> is a way to store and organize data in order to facilitate access and modifications. When studying algorithms we should keep in mind that computers are not infinitely fast, and memory is inexpensive, but it is not free. Hence, <strong>computing time</strong> and <strong>memory</strong> are bounded resources. Different algorithms that claim to solve the same problem can differ in their efficiency.
Thus we can define the order of growth of an algorithm, which gives a characterization of the algorithm's efficiency. We shall assume that we're using a generic processor, random-access machine, where basic arithmetic instructions (<em>add</em>, <em>subtract</em>, <em>multiply</em>, <em>divide</em>, <em>remainder</em>, <em>floor</em>, <em>ceiling</em>), data movement (<em>load</em>, <em>store</em>, <em>copy</em>), and control (conditional and unconditional branch, subroutine call and return) take a <strong>constant</strong> amount of time.  So the <strong><em>random-access machine</em></strong> model is our model of computation. It specifies what operations can be done in an algorithm and how much they cost. </p>
<h3 id="asymptoticnotation">Asymptotic Notation</h3>
<p>We can determine the exact running time of an algorithm, but the extra precision is not usually worth the effort of computing it. The constants and lower order terms are dominated by the effect of the input size. We can study the asymptotic efficiency of algorithms when looking at input sizes that are large enough to make only the order of growth of the running time relevant. With <strong>Asymptotic</strong>, we want to say that we look at input sizes large enough to make only the order of growth of
the running time relevant.  Therefore, in that case, we are concerned about <strong>how the running time of the algorithm increases, as the size of the input increases</strong>. Usually, algorithms that are asymptotically more efficient will be the best choice for all but very small inputs. </p>
<ul>
<li>The most common notation is denominated as <strong>Big-O</strong>, and it describes the worst case. It provides an <em>asymptotic upper bound</em> for the growth rate of runtime of an algorithm. It can be translated in:
O(g(n)) = <span>{</span> f(n): if there exists positive constants c and n<sub>0</sub> such that 0 <span>≤</span> <code>f(n)</code> <span>≤</span> <code>cg(n)</code> for all n <span>≥</span> n<sub>0</sub> <span>}</span>.</li>
<li><strong>Big-Omega</strong>, commonly written as Ω, is an asymptotic notation for the best case. It provides the <em>asymptotic lower bound</em>  for the growth rate of runtime of an algorithm. It can be translated in:
Ω(g(n)) = <span>{</span> f(n): if there exists positive constants c and n<sub>0</sub> such that 0 <span>≤</span> <code>cg(n)</code> <span>≤</span> <code>f(n)</code> for all n <span>≥</span> n<sub>0</sub> <span>}</span>.</li>
<li><strong>Theta</strong>, commonly written as Θ, is an Asymptotic Notation to denote the asymptotically tight bound on the growth rate of runtime of an algorithm. This notation asymptotically bounds a function from above and below. It can be translated in:
Θ(g(n)) = <span>{</span> f(n): if there exists positive constants c<sub>1</sub>, c<sub>2</sub> and n<sub>0</sub> such that 0 <span>≤</span> c<sub>1</sub><code>g(n)</code> <span>≤</span> <code>f(n)</code> <span>≤</span> c<sub>2</sub><code>g(n)</code> for all n <span>≥</span> n<sub>0</sub> <span>}</span>. Which means that g(n) must be Ω(g(n)) and O(g(n)) at the same time. </li>
<li><strong>o-notation</strong> is used to denote an upper bound that is not asymptotically tight. The asymptotic upper bound provided by O-notation may or may not be asymptotically tight. </li>
<li><strong>ω-notation</strong>, similarly to the previous annotation, denotes a lower bound that is not asymptotically tight.</li>
</ul>
<h3 id="datastructures">Data Structures</h3>
<ol>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#arrays">Arrays</a></li>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#linked-lists">Linked Lists</a></li>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#stacks">Stacks</a></li>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#queues">Queues</a></li>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#hash-tables">Hash Tables</a></li>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#binary-search-trees">Binary Search Trees</a></li>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#binary-heaps">Binary Heaps</a></li>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#graphs">Graphs</a></li>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#fibonacci-heaps">Fibonacci Heaps</a></li>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#van-emde-boas-trees">van Emde Boas Trees</a></li>
</ol>
<h4 id="arrays">Arrays</h4>
<p>An array is a contiguous region of memory, broken down into elements, indexed by contiguous integers. We can use arithmetic operations to figure out where we want to perform a read or write, hence it is possible to execute these actions in constant time. By looking at the picture below, we can see that in this example, the array is split into elements of 32 bits. Assuming that we're working with a <strong>0-indexed</strong>, we can access the i <em>-th</em> element by calculating its address in memory: </p>
<div>
<p align="center">
    <span>i-th element address = initial address + element size × i</span>
</p>
<br>
<p align="center">
  <img src="./README.md_files/array.png">
</p>
</div>
<p>A <strong>multi-dimensional</strong> array is formed by an array that contains pointers/references to multiple arrays. Therefore, a <em>10 by 10</em> two-dimensional array would contain an array of 10 pointers, to 10 different arrays of size 10. </p>
<p><br></p>
<p>A <strong>resizable array</strong> is a dynamic array that is initialized with <strong>n</strong> elements. This data structure has the ability of shrinking or growing as the number of elements varies.</p>
<p><br></p>
<p><strong>Challenge</strong>: Implement a <strong>generic</strong> resizable array with the following methods:</p>
<ul>
<li>[ ] <code>size()</code> which retrieves the number of items</li>
<li>[ ] <code>capacity()</code> which retrieves the total capacity</li>
<li>[ ] <code>isEmpty()</code></li>
<li>[ ] <code>get(int index)</code> which retrieves an element at an arbitrary index, launching an exception if <code>outOfBounds</code></li>
<li>[ ] <code>pushBack(T item)</code></li>
<li>[ ] <code>insert(T item, int index)</code> </li>
<li>[ ] <code>pop()</code> which removes the item at the top of the stack and returns its value</li>
<li>[ ] <code>delete(int index)</code></li>
<li>[ ] <code>resize(int newCapacity)</code> as a private method</li>
</ul>
<p>The time complexity for adding or removing elements should be <strong>O(n)</strong>, and <strong>O(1)</strong> for the <code>get(int index)</code> method. 
The memory complexity should be <strong>O(n)</strong>.</p>
<h4 id="linkedlists">Linked Lists</h4>
<p>A <strong>Linked List</strong> is a data structure in which the objects are arranged in linear order. How this data structure, however, is completely different. It is not guaranteed that every item is stored in a contiguous block of memory. The order is determined by a pointer in each item. In a <strong><em>doubly linked list</em></strong>, each item should have tow pointer attributes, pointing to the next and previous item. If the pointer to the previous item is <code>NIL</code>, then the element has no predecessor and is the first element. On the other hand, if the pointer to the next item is <code>NIL</code>, then the element has no successor, and therefore is the last element of the list. </p>
<div>
<p align="center">
  <img src="./README.md_files/doubly.png">
</p>
</div>
<p>A list may have one of several forms. It may be either singly linked our doubly linked, it may be sorted or not, and it may be circular or not. In a <strong><em>singly linked list</em></strong> we omit the pointer to the previous item in each node. </p>
<div>
<p align="center">
  <img src="./README.md_files/singly.png">
</p>
</div>
<p>A <strong><em>circular linked list</em></strong> is based on <strong><em>doubly linked lists</em></strong>, with the exception that the pointer to the previous element in the <em>head</em> node points to the tail of the list. Following the same idea, the pointer to the next element in the <em>tail</em> node points to the head of the list.</p>
<div>
<p align="center">
  <img src="./README.md_files/circular.png">
</p>
</div>
<p>Because of the memory architecture of this data structure, we can't perform accesses/searches in O(1). We would have to check every node in the worst case, therefore the time complexity of a search operation is <strong>O(n)</strong>. Since the insert operation attaches the Node to be inserted in the front of the list, then this operation runs in <strong>O(1)</strong>. 
By making use of <strong>Sentinels</strong>, we can simplify our boundary conditions. A <strong><em>sentinel</em></strong> is a dummy object that points to <em>head</em> and <em>tail</em> of the list, hence we have a <strong><em>circular, doubly linked list with a sentinel</em></strong>. This change can make the <em>Linked List</em> implementation simpler. </p>
<p><strong>Challenge</strong>: Implement a singly and doubly LinkedList object. The object should include the following methods:</p>
<ul>
<li>[ ] <code>size()</code></li>
<li>[ ] <code>isEmpty()</code></li>
<li>[ ] <code>valueAt(int index)</code> which retrieves the value of the node</li>
<li>[ ] <code>pushFront(T value)</code></li>
<li>[ ] <code>popFront()</code></li>
<li>[ ] <code>pushBack(T value)</code></li>
<li>[ ] <code>popBack()</code></li>
<li>[ ] <code>head()</code></li>
<li>[ ] <code>tail()</code></li>
<li>[ ] <code>remove(int index)</code></li>
<li>[ ] <code>reverse()</code></li>
</ul>
<h4 id="stacks">Stacks</h4>
<p>In a <strong>stack</strong>, the element deleted is always the one that has been most recently inserted. Therefore, this data structure implements a <strong><em>last-in, first-out</em></strong> or <strong><em>LIFO</em></strong> policy. A stack, usually has 2 defined methods. The <code>insert</code> operation is often called <code>PUSH</code>, and the <code>delete</code> operation, which retrieves the element that has been most recently inserted, is called <code>POP</code>. Hence, this data structure is an allusion to a real stack, where we remove the top element first. </p>
<p><br></p>
<p>Stacks can be implemented either with a resizable array, or a linked list, where the top elements contain a pointer to the bottom elements. The image bellow is a representation of the second implementation, with 3 elements. </p>
<div>
<p align="center">
  <img src="./README.md_files/stack.png">
</p>
</div>
<p><br></p>
<p><strong>Challenge</strong>: Implement a stack by making use of the previously implemented <strong>resizable array</strong> object. The implementation should include the following methods:</p>
<ul>
<li>[ ] <code>peek()</code> which retrieves the item at the top of the stack</li>
<li>[ ] <code>empty()</code></li>
<li>[ ] <code>push(T item)</code></li>
<li>[ ] <code>pop()</code> which removes the item at the top of the stack and returns its value</li>
</ul>
<p>All methods should have a time complexity of <strong>O(1)</strong>. Since we are using a resizable array, the memory complexity should be <strong>O(n)</strong>.</p>
<h4 id="queues">Queues</h4>
<p>Similarly to the previous section, in a queue, the deleted element is always the one that has been in the set for the longest time: the queue implements a <strong><em>first-in, first-out</em></strong>, or <strong><em>FIFO</em></strong>, policy. The <code>insert</code> operation is often called <code>ENQUEUE</code>, and the <code>delete</code> operation is called <code>DEQUEUE</code>. The queue implementation has a <strong>head</strong> and a <strong>tail</strong>, and it can be implemented with a <em>linked list</em> or an array (<em>circular buffer</em>). The first strategy should include a linked list with a tail pointer. </p>
<p><strong>Challenge</strong>: Implement a queue object by following both strategies. The implementation should include the following methods:</p>
<ul>
<li>[ ] <code>enqueue(T item)</code></li>
<li>[ ] <code>dequeue()</code></li>
<li>[ ] <code>empty()</code></li>
<li>[ ] <code>full()</code> (in the circular buffer implementation)</li>
</ul>
<h4 id="hashtables">Hash Tables</h4>
<p>A <strong><em>Hash Table</em></strong> is an effective data structure for implementing dictionaries and it supports <strong>three</strong> basic operations: <code>INSERT</code>, <code>SEARCH</code> and <code>DELETE</code>. Although searching in a <strong><em>Hash Table</em></strong> can take as long as searching for an element in a <em>Linked List</em> - <em>O(n)</em> - in practice, hashing performs extremely well. Under reasonable assumptions, the average time to search for an element in this data structure is <strong>O(1)</strong>. A <strong><em>Hash Table</em></strong> directly addresses into an ordinary array, making a good use of the possibility of examining an arbitrary position of the array in <strong>O(1)</strong> time. </p>
<p><br></p>
<p>We can take advantage of direct addressing when we can afford to allocate an array that has one position for every possible key. But this situation is often impractical, as it creates some problems. if the universe U is large, storing a table T of size |U| may be impractical, or even impossible, given the memory available on a typical computer. Furthermore, the set K of keys actually stored may be so small relative to U that most of the space allocated for T would be wasted. With hashing, an element is stored in slot <code>h(k)</code>, where <strong>h</strong> is the <strong><em>hash function</em></strong> of <strong><em>k</em></strong>. Hence, the <strong><em>hash function</em></strong> maps the universe <em>U</em> of keys into the slots of the <strong><em>Hash Table</em></strong> <em>T[0…m - 1]</em>: </p>
<p><br></p>
<p><em>h :</em> <span> U → {0, 1, …, m - 1 }</span></p>
<p><br></p>
<p>where the size <em>m</em> is typically much less than |<em>U</em>|. When an element with key k hashes to slot <em>h(k)</em> we also say that <em>h(k)</em> is the hash value of key <em>k</em>. The hash function reduces the range of array indices and hence the size of the array. Instead of size |<em>U</em>|, the array can have size <em>m</em>. The previous idea can be represented in the following image.</p>
<div>
<p align="center">
  <img src="./README.md_files/hash-table.png">
</p>
</div>
<p>However, two keys may hash to the same slot, we call this situation a <strong><em>collision</em></strong>. We can try to avoid collisions altogether by choosing a suitable hash function <em>h</em>. One idea is to make <em>h</em> appear to be random, but because |<em>U</em>| &gt; m, there must be at least to keys that produce the same hash. A well-designed "random"-looking hash function can minimize the number of collisions, but we still need a method for resolving possible collisions. </p>
<h5 id="chaining">Chaining</h5>
<p>In <strong><em>chaining</em></strong>, elements with the same hash are inserted into the same <em>linked list</em>. We can see a representation in the image bellow. The <strong><em>i</em></strong> <em>-th</em> slot contains a pointer to the head of the list of all stored elements that hash to <strong><em>i</em></strong>. If no elements are stored in <strong><em>i</em></strong>, then it contains <code>NIL</code>. Therefore, the running time for insertion is <em>O(1)</em> if we assume that the element is not already present in the hash table. If we do want to make sure that this element is not inserted, then the running time is proportional to the length of the list. We can delete an element in <em>O(1)</em> time if the lists are doubly linked.</p>
<div>
<p align="center">
  <img src="./README.md_files/chaining.png">
</p>
</div>
<p>How well does this method work? We can define the <strong><em>load factor</em></strong> α for <em>T</em> as n / m, the average number of elements stored in a chain. The worst-case behaviour of hashing with chaining is terrible: all <em>n</em> keys hash to the same slot, creating a list of length <em>n</em>. The average case of hashing depends on how well the hash function <em>h</em> distributes the entries among m slots. We can make an assumption that a key is equally likely to hash into any of the <em>m</em> slots. This assumption is called <strong><em>simple uniform hashing</em></strong>. We are also assuming that the hash function computes in constant time. With such assumptions, successful and unsuccessful searches take an average time of <em>Θ(1 + α)</em>. This means that if the number of <strong><em>Hash Table</em></strong> slots is at least proportional to the number of elements in the table then α = <em>n</em> / <em>m</em> = <em>O(1)</em>. Thus, we can support all operations in O(1) time on average.</p>
<h5 id="hashfunctions">Hash Functions</h5>
<p>A good hash function satisfies the assumption of <strong><em>simple uniform hashing</em></strong>. In practice, we can often employ heuristic techniques to create hash functions that perform well. Qualitative information about the distribution of keys may be useful in this design process. Most hash functions assume that the universe of keys is in the set of natural numbers. If the keys are not natural numbers, then we find a way of interpret them as natural numbers. </p>
<p><br></p>
<p>In the <strong><em>division method</em></strong>, the hash function is, <em>h(k) = k mod m</em>, where k is the key that we want to hash. The choice of <em>m</em>, which will be the size of the array, is important, and certain values seem to work better. A prime not too close to an exact power of 2 is often a good choice.</p>
<p><br></p>
<p>The <strong><em>multiplication method</em></strong> can be performed in two steps. First, we multiply the key <em>k</em> by a constant <em>A</em> in the range 0 &lt; <em>A</em> &lt; 1 and extract the fractional part of <em>kA</em>. Then, we multiply this value by <em>m</em> and take the floor of the result. In short, the hash function is the whole part of: <em>h(k) = m(kA mod <span>2<sup>w</sup></span>) &gt;&gt; <span>2<sup>r- w</sup></span></em> where <em><span>2<sup>w</sup></span> = m</em>. An advantage of this method is that the value of <em>m</em> is not critical. Even though that this method works "reasonably" well, it works better for some values than others. </p>
<div>
<p align="center">
  <img src="./README.md_files/multiplication.png">
</p>
</div>
<p><strong><em>Universal hashing</em></strong> consists on choosing the hash function randomly from a set of functions. Because we randomly select the hash function, the algorithm can behave differently even for the same input, guaranteeing good average-case performance for any input. Let <strong><em>H</em></strong> be a finite collection of hash functions that map a given universe <strong><em>U</em></strong> of keys into the range <em>{0, 1, …., m - 1}</em>. We call this collection <strong>universal</strong> if for each pair of distinct keys <em>k, l <span>∈</span> U</em>, the number of hash functions <em>h <span>∈</span> H</em> for which <em>h(k) = h(l)</em> is at most <em>|H| / m</em>. Which basically means that with a hash function randomly chosen from <strong><em>H</em></strong>, the chance of a collision between distinct keys <strong><em>k</em></strong> and <strong><em>l</em></strong> is no more than the chance <em>1 / m</em> of a collision if <em>h(k)</em> and <em>h(l)</em> were randomly and independently chosen from the set <em>{0, 1, …., m - 1}</em>. </p>
<h5 id="openaddressing">Open Addressing</h5>
<p>In <strong><em>Open Addressing</em></strong>, all elements are in the table itself. Each table element contains either an element or <code>NIL</code>, therefore we avoid pointers and chaining solutions. To perform an insertion we successively <strong><em>probe</em></strong> the hash table until we find an empty slot. Hence, with <strong><em>Open Addressing</em></strong> we require that for every key <em>k</em>, there's a <strong><em>probe sequence</em></strong>: <em>{ h(k, 0), h(k, 1), …, h(k, m - 1) }</em>. The following list describes the several operations:</p>
<ul>
<li><code>INSERT(k, v)</code>: Keep probing until an empty slot is found, and insert item when found.</li>
<li><code>SEARCH(k)</code>: As long as the encountered slots are occupied by keys <em><span>≠</span> k</em>, keep probing until you either encounter <em>k</em> or find an empty slot. </li>
<li><code>DELETE(k)</code>: Replace item by <code>DELETED</code> flag. This flags represents an empty item, but it's not a stop condition for <code>INSERT</code> or <code>SEARCH</code>.  </li>
</ul>
<p>There are several types of probing, </p>
<ul>
<li><strong><em>Linear Probing</em></strong>: <em>h(k) = (h'(k) + i) mod m</em>. THis method suffers from a problem known as <strong><em>primary clustering</em></strong>. Long runs of occupied slots build up, increasing the average search time.</li>
<li><strong><em>Quadratic Probing</em></strong>: <em>h(k) = (h'(k) + c<span><sub>1</sub></span>i + c<span><sub>2</sub></span>i<span><sup>2</sup></span>) mod m</em>, where <strong><em>c<span><sub>1</sub></span></em></strong> and <strong><em>c<span><sub>2</sub></span></em></strong> are positive auxiliary constants, and <strong><em>i</em></strong> corresponds to the <strong><em>i</em></strong> <em>-th</em> probe. Therefore, the first hash function corresponds to <em>h'(k)</em>. This property leads to a milder form of clustering, called <strong><em>secondary clustering</em></strong>. The initial probe determines the entire sequence, and so only <em>m</em> distinct probe sequences are used. </li>
<li><strong><em>Double Hashing</em></strong>: <em>h(k) = (h<span><sub>1</sub></span>(k) + i h<span><sub>2</sub></span>(k)) mod m</em>. This methods is one of the bests available for Open Addressing. In this case, the first position is again given by <em>h<span><sub>1</sub></span>(k)</em>, and successive probes are offset from previous positions by the amount <em>h<span><sub>2</sub></span>(k)</em>, modulo <em>m</em>. The probe sequence here depends in two ways upon the key <em>k</em>. The value <em>h<span><sub>2</sub></span>(k)</em> must be relatively prime to <em>m</em>.</li>
</ul>
<h5 id="perfecthashing">Perfect Hashing</h5>
<p>Hashing offers an excellent <em>average-case</em> performance, but the previous cases offer a not so good <em>worst-case</em> performance. This scenario can change if the set of keys is <strong><em>static</em></strong>, where we can apply a hashing technique <strong><em>Perfect Hashing</em></strong> where we can perform a <code>SEARCH</code> operation in <strong><em>constant time</em></strong>. The idea is to use <strong><em>two levels of hashing</em></strong>, with universal hashing at each level. The first level is the same as the first level of <em>hashing with chaining</em>. We want to hash <em>n</em> keys into <em>m</em> slots using a hash function <em>h</em> selected from a family of universal hash functions. In the second level, we avoid linked list associated with the <strong><em>i</em></strong> <em>-th</em> entry, and we use a small <strong><em>secondary hash table</em></strong> instead, where we use a second hash function. By choosing this hash function carefully, we can avoid collisions. The size of the secondary hash table is the square of the number of keys hashing to the <strong><em>i</em></strong> <em>-th</em> entry. The choice of squaring this value can be supported by the <strong><em>birthday paradox</em></strong>:</p>
<div style="border-radius: 25px; border: 2px solid gray;">
<p style="margin-top: 13px" align="center">
  If I have n people with <span>n<sup>2</sup></span> possible birthdays, the probability of having 2 people with the same birthday would be 1 / 2
</p>
</div>
<p><br></p>
<p>Therefore, we would expect to have less collisions in the second level hash table. The following image contains a representation of <strong><em>Perfect Hashing</em></strong>.</p>
<div>
<p align="center">
  <img height="60%" "="" width="60%" src="./README.md_files/perfect-hashing.png">
</p>
</div>
<p><strong>Challenge</strong>: Implement a hash table with <strong><em>Open Addressing</em></strong>, following a <strong><em>linear probing</em></strong> strategy.  </p>
<ul>
<li>[ ] <code>hash(K key, int m)</code></li>
<li>[ ] <code>add(K key, V value)</code></li>
<li>[ ] <code>get(K key)</code></li>
<li>[ ] <code>exists(K key)</code></li>
<li>[ ] <code>remove(K key)</code></li>
</ul>
<h4 id="binarysearchtrees">Binary Search Trees</h4>
<p>This data structure supports operations including <code>SEARCH</code>, <code>MINIMUM</code>, <code>MAXIMUM</code>, <code>PREDECESSOR</code>, <code>SUCCESSOR</code>, <code>INSERT</code> and <code>DELETE</code>. Basic operations take time proportional to the height of the tree. For a complete binary tree, with <em>n</em> nodes, such operations run in <strong>O(log n)</strong> time. But if we have a linear chain of nodes, the same operations takes <strong>O(n)</strong> time. We can represent this data structure by a linked data structure in each which each node is an object. These nodes contain more attributes like <strong>left</strong>, <strong>right</strong> and <strong>parent</strong>, that point to the its left and right child node, and its parent node respectively. The root node is the only node whose <strong>parent</strong> is <code>NULL</code>. This data structure must always satisfy the <strong><em>binary-search-tree property</em></strong>:</p>
<div style="border-radius: 25px; border: 2px solid gray;">
<p style="margin-top: 13px; margin-right: 15px; margin-left: 15px;" align="center">
  Let <span><b>x</b></span> be a node in a binary search tree. <span><b>y</b></span> is a node in the left subtree of <span><i>x</i></span> if <span>key(<i>y</i>) ≤ key(<i>x</i>)</span>. <span><b>y</b></span> is a node in the right subtree of <span><i>x</i></span> if <span>key(<i>y</i>) ≥ key(<i>x</i>)</span> 
</p>
</div>
<p><br></p>
<p>Therefore, a <code>SEARCH</code> operation takes <strong>O(h)</strong>, with <strong>h</strong> being the height of the tree. We can define the height of a node, by calculating the longest path of that node to a leaf. If we want to find the node with the smallest key, a.k.a. to perform a <code>MINIMUM</code> operation, then we have to successively follow left child pointers. Whereas if we want to the node with the largest key, or perform a <code>MAXIMUM</code> operation, then we should follow the right child pointers. These operations also run in <strong>O(h)</strong>. The same happens for the <code>PREDECESSOR</code> and <code>SUCESSOR</code> operations. </p>
<p><br></p>
<p>The <code>INSERT</code> and <code>DELETE</code> operations cause the tree to change. Even though that the <code>INSERT</code> operation must follow the binary-search-tree property, this operation is relatively straight forward. Handling the <code>DELETE</code> operation can be more difficult. Deleting a node <strong>z</strong> from a binary-search-tree <strong>T</strong> has three cases:</p>
<ul>
<li>If <strong>z</strong> has no children, then we simply remove it by modifying its parent to replace <strong>z</strong> as <code>NULL</code> as its child. </li>
<li>If <strong>z</strong> has one child, then we elevate that child to take <strong>z</strong>'s position in the parent node child pointer. </li>
<li>If <strong>z</strong> has two children, then we find <strong>z</strong>'s successor <strong><em>y</em></strong>, which must be in <strong>z</strong>'s right subtree, and have <strong><em>y</em></strong> take <strong>z</strong>'s position in the tree. The rest of <strong>z</strong>'s original right subtree, becomes <strong><em>y</em></strong>'s  new right subtree. <strong>z</strong>'s left subtree becomes <strong><em>y</em></strong>'s new left subtree. </li>
</ul>
<p>Both last operations run in <strong>O(h)</strong> time as well. </p>
<p><strong>Challenge</strong>: Implement a binary-search-tree with the following functionalities.  </p>
<ul>
<li>[ ] <code>insert(K key)</code></li>
<li>[ ] <code>exists(K key)</code></li>
<li>[ ] <code>size()</code></li>
<li>[ ] <code>min()</code></li>
<li>[ ] <code>max()</code></li>
<li>[ ] <code>predecessor(K key)</code></li>
<li>[ ] <code>successor(K key)</code></li>
<li>[ ] <code>delete(K key)</code></li>
</ul>
<h5 id="avltrees">AVL trees</h5>
<p>A Binary Search Tree is balanced if you can ensure that its height is Θ(log n). AVL trees are balanced, and they ensure it by defining the <strong><em>AVL-property</em></strong>:</p>
<div style="border-radius: 25px; border: 2px solid gray;">
<p style="margin-top: 13px; margin-right: 15px; margin-left: 15px;" align="center">
  AVL trees require heights of left and right children of <b>every</b> node to differ by at most <b>1</b>.
</p>
</div>
<p><br></p>
<p>We can prove that this property ensures that the height of the tree is Θ(log n), and therefore all operations take <strong>O(log n)</strong> time. We can start by supporting this claiming, by defining <span>N<sub>h</sub></span> to be the <em>minimum number of nodes in an AVL tree of height</em> <strong><em>h</em></strong>. And therefore:</p>
<div>
<p style="margin-top: 13px; margin-right: 15px; margin-left: 15px;" align="center">
  <span>N<sub>h</sub></span> = 1 + <span>N<sub>h - 1</sub></span> + <span>N<sub>h - 2</sub></span>
</p>
</div>
<p>by keeping in mind that the <strong><em>Fibonacci sequence</em></strong> can be defined by, </p>
<div>
<p style="margin-top: 13px; margin-right: 15px; margin-left: 15px;" align="center">
  <span>F<sub>h</sub></span> = <span>F<sub>h - 1</sub></span> + <span>F<sub>h - 2</sub></span>
</p>
</div>
<p>we know that,</p>
<div>
<p style="margin-top: 13px; margin-right: 15px; margin-left: 15px;" align="center">
  <span>N<sub>h</sub></span> &gt; <span>F<sub>h</sub></span> and <span>N<sub>h</sub> = n</span> and <span>F<sub>h</sub></span> = <span>φ<sup>h</sup> / √5</span>
</p>
</div>
<p>which means that,</p>
<div>
<p style="margin-top: 13px; margin-right: 15px; margin-left: 15px;" align="center">
  <span>n &gt; φ<sup>h</sup> / √5 ⇔ h &lt; log<sub>φ</sub>n ⇔ h &lt; 1.44 log n</span>
</p>
</div>
<p>and therefore the <strong><em>height</em></strong> of the tree is order of log n.</p>
<p>Therefore, we have to ensure that this property stands every time we <code>INSERT</code> or <code>DELETE</code> a value in our <strong>Binary Search Tree</strong>. We can achieve that by making use of operations that are denominated as <strong><em>rotations</em></strong>, which take <strong>constant time</strong>. A node can rotate right or left, by changing its pointer structure. A right rotation can be reverted by applying a left operation and vice versa. </p>
<div>
<p align="center">
  <img height="50%" "="" width="50%" src="./README.md_files/rotations.png">
</p>
</div>
<p>If the <strong><em>AVL tree property</em></strong> is violated, we know that somewhere in our tree, a node has a left child and right child whose height differs by 2. The simple case comes when both sub nodes are left childs or right childs of their parents, and we can solve the situation by rotating the node of <strong>height 1</strong>.  </p>
<div>
<p align="center">
  <img height="55%" "="" width="55%" src="./README.md_files/simple-rotation.png">
</p>
</div>
<p>Having alternated left, right or right, left childs on the penultimate and last levels creates a peculiar problem, which can be solved by applying a <strong>double-rotation</strong>. This rotation can be followed in the picture below.</p>
<div>
<p align="center">
  <img height="70%" "="" width="70%" src="./README.md_files/double-rotation.png">
</p>
</div>
<p><strong>Challenge</strong>: Based on the previously developed binary-search-tree, implement an AVL tree whose insert and delete respect the <strong><em>AVL tree property</em></strong>.  </p>
<ul>
<li>[ ] <code>insert(K key)</code></li>
<li>[ ] <code>delete(K key)</code></li>
</ul>
<h5 id="redblacktrees">Red-Black Trees</h5>
<p>Similarly to AVL trees, <strong><em>Red-Black Trees</em></strong> are one of many search trees that are balanced, thus by guaranteeing that their height is of of order <strong><em>log n</em></strong>, they guarantee that basic dynamic-set operations take <strong>O(log n)</strong> time in the worst case. A <strong><em>Red-Black Trees</em></strong> is a binary search tree with an extra <strong>color</strong> bit per node, which can be <em>RED</em> or <em>BLACK</em>. By constraining the node colours on any simple path from the root to the leafs, they can make the tree approximately <strong><em>balanced</em></strong>. A red-black tree is a <em>binary search tree</em> that satisfies the following properties:</p>
<ol>
<li>Every node is either  <b style="color: #B22222;">red</b> or <b>black</b>.</li>
<li>The root is <b>black</b>.</li>
<li>Every leaf (NULL) is <b>black</b>.</li>
<li>If a node is <b style="color: #B22222;">red</b>, then both its children are <b>black</b>.</li>
<li>For each node, all simple paths from the node to descendant leaves contain the same number of <b>black</b> nodes.</li>
</ol>
<p>For convenience, we use a sentinel to represent <code>NULL</code> nodes when dealing with boundary conditions. These nodes are black. We call the number of black nodes on any simple path, from a node <strong>X</strong> down to a leaf, the <strong><em>black-height</em></strong> of the node. This convention is a notion supported by <strong>property 5</strong>. The <strong><em>black-height</em></strong> of a <strong><em>Red-Black Tree</em></strong> is defined by the <strong><em>black-height</em></strong> of the root node. We shall prove the following claim:</p>
<div style="border-radius: 25px; border: 2px solid gray;">
<p style="margin-top: 13px; margin-right: 15px; margin-left: 15px;" align="center">
  A Red-Black tree with <b>n</b> internal nodes has height of at most <i>2 log (n + 1)</i>
</p>
</div>
<p>Let's call the <strong><em>black-height</em></strong> of a node <strong><em>h</em></strong>', and the original height <strong><em>h</em></strong>. We can deduct that, with black nodes only (standing for a valid Red-Black tree), the number of node is of least,</p>
<div>
<p style="margin-top: 13px; margin-right: 15px; margin-left: 15px;" align="center">
  <span>2<sup>h'</sup>-1</span>
</p>
</div>
<p>This means that,</p>
<div>
<p style="margin-top: 13px; margin-right: 15px; margin-left: 15px;" align="center">
  <span>2<sup>h'</sup>-1 ≤ n ⇔ h' ≤ log (n + 1)</span>
</p>
</div>
<p>But because <strong><em>property 3</em></strong>, we know that,</p>
<div>
<p style="margin-top: 13px; margin-right: 15px; margin-left: 15px;" align="center">
  <span>h ≤ 2h'</span>
</p>
</div>
<p>Which implies that,</p>
<div>
<p style="margin-top: 13px; margin-right: 15px; margin-left: 15px;" align="center">
  <span>h / 2 ≤ log (n + 1) ⇔ h ≤ 2 log (n + 1)</span>
</p>
</div>
<p>and therefore these trees offer an height function of logarithmic order. As a consequence of this proof, since each operation can run in <strong><em>O(h)</em></strong> in a binary search tree of height <strong>h</strong>, we can implement the dynamic-set of operations in <strong>O(log n)</strong> time.</p>
<p>To insert a node into an n-node red-black tree, we first insert node <strong><em>z</em></strong> into the tree <strong><em>T</em></strong> using the default binary search tree insertion method, and colouring the node with <b style="color: #B22222;">red</b>. To guarantee that the red-black tree properties stand, we use an auxiliary method to recolour nodes our perform rotations. Properties can be violated within 3 different cases:</p>
<div>
<p align="center">
  <img height="70%" "="" width="70%" src="./README.md_files/case1ins.png">
</p>
</div>
<h6 id="case1xsuncleyisred">Case 1: x's uncle y is red</h6>
<p>In the first case, we've inserted a node <strong>X</strong> with value <em>b</em>, which is a child and nephew of a <b style="color: #B22222;">red</b> node. <strong>Property 4</strong> is now being violated.  We can fix it by colouring both <strong><em>a</em></strong> and <strong><em>d</em></strong> with <b>black</b>. But because node <strong><em>c</em></strong> is black, this move violates <strong>property 5</strong>, hence we have to colour node <strong><em>c</em></strong> with <b style="color: #B22222;">red</b>. Because we've assured that the sub-tree with root <strong><em>c</em></strong> is now respecting all red-black tree properties, we're going to propagate <strong><em>X</em></strong> to the upper levels in order to make sure that <strong><em>c</em></strong> is now respecting these properties as well (not breaking, for example, property 4).</p>
<div>
<p align="center">
  <img height="70%" "="" width="70%" src="./README.md_files/case2ins.png">
</p>
</div>
<h6 id="case2xsgrandparentyisblackandxisarightchild">Case 2: x's grandparent y is black, and x is a right child</h6>
<p>In this case, we could to solve the situation in one step if <strong><em>x</em></strong> was a left child. Therefore we perform a left rotation of node <strong><em>a</em></strong>, encountering case 3.</p>
<div>
<p align="center">
  <img height="70%" "="" width="70%" src="./README.md_files/case3ins.png">
</p>
</div>
<h6 id="case3xsgrandparentyisblackandxisarightchild">Case 3: x's grandparent y is black, and x is a right child</h6>
<p>Whenever we encounter this case, we are violating <em>property 5</em>. We can preserve it by rotating <strong>b</strong> and colouring it <b>black</b>. Followed by colouring both <strong>a</strong> and <strong>c</strong> with <b style="color: #B22222;">red</b>. And since we don't have two <b style="color: #B22222;">red</b> nodes in a row, we've completed the procedure. </p>
<p>We have to cover the mirrored scenario as well, where we execute the same procedure with <strong>left</strong> and <strong>right</strong> exchanged. Thus, we have three more trivial cases, which follow the idea above. </p>
<p>The deletion process for red-black trees is a bit more complicated than the insertion process. We proceed with the default binary search tree deletion, and this time we have 4 cases to cover. </p>
<h6 id="case1xssiblingwisred">Case 1: x's sibling w is red</h6>
<p>This situation violates the <strong>5</strong>th property of red-black trees properties. We can switch the colours of <strong><em>w</em></strong> and <strong><em>x's parent</em></strong>, and then perform a left-rotation on <strong><em>x's parent</em></strong>. Hence, this case is now converted to a case 2, 3 or 4. </p>
<div>
<p align="center">
  <img height="70%" "="" width="70%" src="./README.md_files/case1del.png">
</p>
</div>
<h6 id="case2xssiblingwisblackandbothofwschildrenareblack">Case 2: x’s sibling w is black, and both of w’s children are black</h6>
<p>To compensate from removing one black from <strong><em>x's parent</em></strong>, we would like to colour <strong><em>w</em></strong> red. To compensate from removing one black from <strong><em>x</em></strong> and <strong><em>w</em></strong> we have to add an extra black node to <strong><em>x's parent</em></strong>, which can be either <b style="color: #B22222;">red</b> or <b>black</b>. </p>
<div>
<p align="center">
  <img height="70%" "="" width="70%" src="./README.md_files/case2del.png">
</p>
</div>
<h6 id="case3xssiblingwisblackwsleftchildisredandwsrightchildisblack">Case 3: x’s sibling w is black, w’s left child is red, and w’s right child is black</h6>
<p>To avoid a property violation, we can switch the colour of <strong><em>w</em></strong> and its left child, and then perform a <em>right-rotation</em> on <strong><em>w</em></strong>. The new sibling <strong><em>w</em></strong> of <strong><em>x</em></strong> is now a black node with a red right
child, hence we have transformed case 3 into case 4.</p>
<div>
<p align="center">
  <img height="70%" "="" width="70%" src="./README.md_files/case3del.png">
</p>
</div>
<h6 id="case4xssiblingwisblackandwsrightchildisred">Case 4: x’s sibling w is black, and w’s right child is red</h6>
<p>By making colour changes and performing a left rotation on <strong><em>x's parent</em></strong> we can remove the extra black on <strong><em>x</em></strong>.</p>
<div>
<p align="center">
  <img height="70%" "="" width="70%" src="./README.md_files/case4del.png">
</p>
</div>
<p><strong>Challenge</strong>: Based on the previously developed binary-search-tree, implement a <strong><em>Red-Black tree</em></strong> whose insert and delete respect the <strong><em>Red-Black tree properties</em></strong>. </p>
<ul>
<li>[ ] <code>insert(K key)</code></li>
<li>[ ] <code>delete(K key)</code></li>
</ul>
<h4 id="binaryheaps">Binary Heaps</h4>
<p>The <a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#heap">heap</a> data structure has many uses. In this section we're going to present the most popular application of a heap, an efficient priority queue. <strong><em>Priority Queues</em></strong> come in two forms, max-priority queues and min-priority queues. We're going to focus on how to implement a max-priority queue. A <strong><em>Priority Queue</em></strong> is a data structure for maintaining a set of elements associated with a <strong><em>key</em></strong>. It supports the following operations:</p>
<ul>
<li><code>max()</code></li>
<li><code>popMax()</code></li>
<li><code>increaseKey(int index, k newKey)</code> which updates the value of a given key to a new value, assuming that <code>newKey</code> is at least as large as the old key.</li>
<li><code>insert(K key)</code></li>
</ul>
<p>By using a heap, we can implement <code>max</code> in <i><b>Θ(1)</b></i>, by simply returning the root of the heap. We can then implement <code>popMax</code> by returning the root, decreasing the heap size, exchanging the root with the last element and call <code>maxHeapify</code> on the new root. The <code>increaseKey</code> procedure can be executed increasing the key of a given <code>index</code>. This procedure can be followed by exchanging this key with its <strong><em>parents</em></strong> in case we have a <strong><em>heap property violation</em></strong>, making it run in <strong><em>O(log n)</em></strong> time. We can then implement <code>insert</code>, by simply increasing the heap size, setting the new element to <code>-∞</code> and then running increase key on the new element with the correct key. Therefore <code>insert</code> runs on <strong><em>O(log n)</em></strong> time as well.</p>
<p><strong>Challenge</strong>: Implement a priority queue based on the <a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#heap">heap</a> implementation.</p>
<h4 id="graphs">Graphs</h4>
<p>When we characterize the running time of a graph <strong><em>G = (V, E)</em></strong> we usually denote <strong><em>V</em></strong> to be the set of vertices and <strong><em>E</em></strong> the set of edges. Hence, to measure the size of each set, we use <strong><em>|V|</em></strong> and <strong><em>|E|</em></strong>. Note that we might say that an algorithm runs in <em>O(V E)</em>, which corresponds to <em>O(|V| |E|)</em>.</p>
<p>We can represent a graph by the <strong><em>adjacency-list representation</em></strong>, which saves an array <em>Adj</em> of <em>|V|</em> lists, one for each vertex. For each <i>u ∈ V</i>, the adjacency list <em>Adj[u]</em> contains all the vertices <em>v</em> such that there is an edge <i>(u, v) ∈ E</i>. Meaning that <em>Adj[u]</em> contains all vertices adjacent to <em>u</em> in <em>G</em>. If <em>G</em> is a directed graph, then the sum of lengths of all adjacency list is <em>|E|</em>. But if <em>G</em> is an undirected graph, then <em>u</em> appears in <em>v</em>'s adjacency list and vice-versa, therefore the sum of all lengths of adjacency is <em>2|E|</em>. In both cases, this representation has a space complexity of <i><b>Θ(V + E)</b></i>. We can also adapt this representation to support <strong><em>weighted graphs</em></strong>. We can store the weight <em>w(u, v)</em> of the edge <i>(u, v) ∈ E</i> with vertex <em>v</em> in <em>u</em>'s adjacency list. In a more <strong><em>object oriented perspective</em></strong> we could store this list inside a <em>Node</em> object, without declaring the array. </p>
<p>For the <strong><em>adjacency-matrix representation</em></strong> we assume that vertices are numbered from <strong><em>0 … |V|</em></strong>, and we have a matrix <i>|V| × |V|</i>, such that <i>a<sub>i, j</sub></i> contains 1 if <i>(i, j) ∈ E</i>, and 0 otherwise. This representation requires <i><b>Θ(V<sup>2</sup>)</b></i>.
You might have an implicit representation as well, where we have <code>Adj(u)</code> as a function, that retrieves the list of vertices that are adjacent to vertex <em>u</em>.</p>
<p>The following picture contains both <strong><em>adjacency-list</em></strong> and <strong><em>adjacency-matrix</em></strong> representations of a <strong><em>unweighted</em></strong> and <strong><em>undirected</em></strong> graph.</p>
<div>
<p align="center">
  <img height="100%" "="" width="100%" src="./README.md_files/adjacency-matrix-list.png">
</p>
</div>
<h5 id="breadthfirstsearch">Breadth First Search</h5>
<p>Given a graph <i>G = (V, E)</i>, and a source vertex <strong><em>s</em></strong>, breadth-first search systematically explores the edges of <em>G</em> to "discover" every vertex that is reachable from <strong><em>s</em></strong>. It computes the distance (smaller number of edges) from <em>s</em> to each reachable vertex. Consequently, it produces a "breadth-first tree" with root <em>s</em>. Thus, for any vertex <em>v</em> that is reachable from <em>s</em>, , the simple path in the "breadth-first tree" from <em>s</em> to <em>v</em> corresponds to the shortest path. This search expands the frontier between discovered and undiscovered vertices uniformly across the breadth of the frontier. The algorithm discovers any vertex at distance <strong><em>k</em></strong> from <em>s</em>, before discovering vertices at distance <strong><em>k + 1</em></strong>. The following snippet includes an implementation in <em>Java</em>.</p>
<pre><code class="Java language-Java hljs lasso"><span class="hljs-keyword">public</span> <span class="hljs-literal">void</span> breadthFirstSearch(Node node) {
    <span class="hljs-built_in">Set</span><span class="hljs-subst">&lt;</span>Node<span class="hljs-subst">&gt;</span> visited <span class="hljs-subst">=</span> <span class="hljs-literal">new</span> HashSet<span class="hljs-subst">&lt;&gt;</span>();
    <span class="hljs-built_in">Queue</span><span class="hljs-subst">&lt;</span>Node<span class="hljs-subst">&gt;</span> <span class="hljs-built_in">queue</span> <span class="hljs-subst">=</span> <span class="hljs-literal">new</span> LinkedList<span class="hljs-subst">&lt;&gt;</span>(Arrays<span class="hljs-built_in">.</span>asList(node));
    <span class="hljs-keyword">while</span> (<span class="hljs-subst">!</span><span class="hljs-built_in">queue</span><span class="hljs-built_in">.</span>isEmpty()) {
        Node <span class="hljs-keyword">parent</span> <span class="hljs-subst">=</span> <span class="hljs-built_in">queue</span><span class="hljs-built_in">.</span>remove();
        for (Node child : <span class="hljs-keyword">parent</span><span class="hljs-built_in">.</span>children) {
            <span class="hljs-keyword">if</span> (<span class="hljs-subst">!</span>visited<span class="hljs-built_in">.</span>contains(child)) {
                visited<span class="hljs-built_in">.</span>add(child);
                <span class="hljs-built_in">queue</span><span class="hljs-built_in">.</span>add(child);
            }
        }
    }
}
</code></pre>
<p>In order to analyse this algorithm, we must notice that a <em>Node</em> is never enqueued/dequeued more than once, and since these operations run in <i>O(1)</i>, we can also say that the total time of queue related operations take <i>O(V)</i>. This procedure also scans the adjacency list of a given vertex at least once. Since the sum of lengths of adjacency lists is <i>Θ(2E)</i> for undirected graphs and <i>Θ(E)</i> for directed graphs, we have that the running time of this algorithm is <i><b>O(V + E)</b></i>. Thus, it runs in linear time.</p>
<p>This procedure builds a <strong><em>breadth-first tree</em></strong>, that contains an unique simple path from <em>s</em> to <em>v</em>.</p>
<h5 id="depthfirstsearch">Depth First Search</h5>
<p>The strategy adapted by this method is, as the name suggests, to go "deeper" in the graph whenever possible. Hence, depth-first search explores edges out of the most recently discovered vertex <em>v</em> that still has unexplored edges leaving it. Once all <em>v</em>'s edges have been explored, the search backtracks to explore edges leaving the vertex from which <strong><em>v</em></strong> was discovered. This process continues until we have discovered all the vertices that are reachable from the original source vertex <strong><em>s</em></strong>. Therefore, we can write the algorithm as follows:</p>
<pre><code class="Java language-Java hljs lasso"><span class="hljs-keyword">public</span> <span class="hljs-literal">void</span> depthFirstSearch(Node node) {
    <span class="hljs-built_in">Set</span><span class="hljs-subst">&lt;</span>Node<span class="hljs-subst">&gt;</span> visited <span class="hljs-subst">=</span> <span class="hljs-literal">new</span> HashSet<span class="hljs-subst">&lt;&gt;</span>();
    <span class="hljs-built_in">Stack</span><span class="hljs-subst">&lt;</span>Node<span class="hljs-subst">&gt;</span> <span class="hljs-built_in">stack</span> <span class="hljs-subst">=</span> <span class="hljs-literal">new</span> <span class="hljs-built_in">Stack</span><span class="hljs-subst">&lt;&gt;</span>();
    <span class="hljs-built_in">stack</span><span class="hljs-built_in">.</span>push(node);
    <span class="hljs-keyword">while</span> (<span class="hljs-subst">!</span><span class="hljs-built_in">stack</span><span class="hljs-built_in">.</span>empty()) {
        Node <span class="hljs-keyword">parent</span> <span class="hljs-subst">=</span> <span class="hljs-built_in">stack</span><span class="hljs-built_in">.</span>pop();
        for (Node child : <span class="hljs-keyword">parent</span><span class="hljs-built_in">.</span>children) {
            <span class="hljs-keyword">if</span> (<span class="hljs-subst">!</span>visited<span class="hljs-built_in">.</span>contains(child)) {
                visited<span class="hljs-built_in">.</span>add(child);
                <span class="hljs-built_in">stack</span><span class="hljs-built_in">.</span>push(child);
            }
        }
    }
}
</code></pre>
<p>We've transformed this algorithm into an iterative one, but we could easily write it by making use of recursion. Notice that if we had a separate node that is not connected to this graph, we would have to iterate through all nodes first in order to perform a complete <strong><em>depth-first search</em></strong>. The running time of this algorithm is still <i><b>O(V + E)</b></i>. An important property of this search is that it has <strong><em>parenthesis structure</em></strong>. Which means that the discovery of vertex <strong><em>v</em></strong> only finishes after the end of its children discovery. Therefore the discovery of the <strong><em>source vertex</em></strong> only finishes after discovering every vertex that are reachable from it. </p>
<p>We can also <strong><em>classify the edges</em></strong> of the input graph <em>G = (V, E)</em>. The type of each edge can be,</p>
<ol>
<li>Tree edge: An edge <em>(u, v)</em> is a tree edge if <em>v</em> was discovered by exploring the edge <em>(u, v)</em>.</li>
<li>Back edge: An edge <em>(u, v)</em> connecting a vertex <em>u</em> to an ancestor <em>v</em> in a depth-first tree. Self loops, which may occur in directed graphs, are back edges.</li>
<li>Forward edge: An edge <em>(u, v)</em> that is connecting a vertex <em>u</em> to a descendant <em>v</em> in a depth-first tree.</li>
<li>Cross edge: An edge <em>(u, v)</em> that can go between vertices in the same depth-first tree., as long as one vertex is not descendant of the other.</li>
</ol>
<p>We now claim that in a depth-first search of an undirected graph <em>G</em>, every edge of <em>G</em> is either a tree edge or a back edge. Meaning that if <em>G</em> has a cycle, then the cycle is formed with a back edge. </p>
<p>We can also use <strong><em>depth-first search</em></strong> to perform a topological sort on a <em>DAG</em> (Directed Acyclic Graph). A <strong><em>topological sort</em></strong> of a dag <i>G = (V, E)</i> is a linear ordering of all its vertices such that if <em>g</em> contains an edge <em>(u, v)</em>, then <em>u</em> appears before <em>v</em> in the ordering. This property is supported by  <em>depth-first search</em> <strong><em>parenthesis structure</em></strong>. Hence, we can use directed acyclic graphs to indicate precedences among events. </p>
<div>
<p align="center">
  <img height="75%" "="" width="75%" src="./README.md_files/topological-sort.png">
</p>
</div>
<p>A topological sort of the previous graph corresponds the order of which vertexes are discovered first, this can be useful in job scheduling, for example. The following snippet describes how one could implement this algorithm.</p>
<pre><code class="hljs livecodeserver">TopologicalSort(G):
    call DFS(G)
    <span class="hljs-built_in">add</span> vertex <span class="hljs-built_in">to</span> <span class="hljs-operator">a</span> linked list <span class="hljs-keyword">as</span> <span class="hljs-keyword">each</span> vertex discovering <span class="hljs-built_in">process</span> comes <span class="hljs-built_in">to</span> <span class="hljs-operator">an</span> <span class="hljs-function"><span class="hljs-keyword">end</span></span>
    revert linked list order
</code></pre>
<p>We can perform a topological sort in time <i><b>Θ(V + E)</b></i>, because we are essentially calling <strong><em>depth-first search</em></strong> and adding elements to a linked-list, which takes <em>constant</em> time. </p>
<h4 id="fibonacciheaps">Fibonacci Heaps</h4>
<p>This data structure supports a set of operations that constitutes what is known as a <strong><em>mergeable heap</em></strong>. We can also state that several operations run in <strong><em>constant amortized time</em></strong>. Apart from all operations that we could do in a binary heap, this data structure supports the <strong><em>union(h1, h2)</em></strong> operation, which creates a returns a new heap containing all elements of <em>h1</em> and <em>h2</em>. </p>
<div>
<p align="center">
  <img height="50%" "="" width="50%" src="./README.md_files/analysis.png">
</p>
</div>
<p>From a theoretical point of view, Fibonacci heaps are specially desirable when the number of extract-min and delete operations is small. From a practical point of view however, the programming complexity of this data structure makes it less desirable than regular heaps. Both <em>binary heaps</em> and <em>Fibonacci heaps</em> are inefficient in how they support the <strong><em>search</em></strong> operation. Like other data structures that we have seen, Fibonacci Heaps are based on rooted trees. We represent each element as a node within a tree, and each node has a <em>key</em> attribute. </p>
<p>The Fibonacci heap is a collection of rooted trees that are <strong><em>min-heap ordered</em></strong>. </p>
<h4 id="vanemdeboastrees">van Emde Boas Trees</h4>
<h3 id="algorithms">Algorithms</h3>
<ol>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#sorting">Sorting</a></li>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#dynamic-programming">Dynamic Programming</a></li>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#graph-algorithms">Graph Algorithms</a><ol>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#minimum-spanning-trees">Minimum Spanning Trees</a></li>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#single-source-shortest-paths">Single Source Shortest Paths</a></li>
<li><a href="http://localhost:3333//mnt/7AAA6049AA60044B/GitHub/computer-science/README.md#maximum-flows">Maximum Flows</a></li></ol></li>
<li>[Karatsuba Algorithm]</li>
</ol>
<h4 id="sorting">Sorting</h4>
<p>In this section we're going to come across several algorithms that solve the <strong><em>sorting problem</em></strong>. This methods take a sequence <strong><em>a</em></strong> of <em>n</em> numbers as an input, and they return all elements in the sequence such as <i>a<sub>0</sub> ≤ a<sub>1</sub> ≤ a<sub>2</sub> ≤ a<sub>3</sub> ≤ a<sub>4</sub> (…)</i>. The input sequence is usually represented with an array, but we can represent it in a linked list as well.  </p>
<h5 id="insertionsorting">Insertion Sorting</h5>
<p>Starting with insertion sort, it was shown before that this algorithm is an efficient algorithm for handling relatively small inputs. In this algorithm we run through an array from left to right, comparing each element to the elements on the left sub array and inserting it into the correct (sorted) position. This algorithm sorts the input <strong><em>in-place</em></strong>, which means that uses a constant number of stored variables at any time. This sorting method is expected to run in <strong><em>Θ(<span>n<sup>2</sup></span>)</em></strong> in the worst case, and has a memory complexity of <i>O(1)</i>.</p>
<div>
<p align="center">
  <img height="75%" "="" width="75%" src="./README.md_files/insertion.png">
</p>
</div>
<p><strong>Challenge</strong>: Implement insertion sort. </p>
<ul>
<li>[ ] <code>insertionSort(T [] array)</code></li>
</ul>
<h5 id="mergesort">Merge Sort</h5>
<p>This algorithm follows a <strong><em>divide and conquer</em></strong> strategy to solve this problem. We start by dividing our sub-domains into two in each step, and then merging each them into the correct order. After reaching to the bottom level (sub-domains with 1 element), we simply pair both sub-domains that were split, inserting elements in the correct order in a new array by executing the "two-finger algorithm". </p>
<div>
<p align="center">
  <img height="30%" "="" width="30%" src="./README.md_files/merge-tree.png">
</p>
</div>
<p>By analysing the recurrence tree below, we can clearly see that we have <strong><em>1 + log n</em></strong> levels with <strong><em>n</em></strong> elements per level. Hence our algorithm is having an <strong><em>C . n</em></strong> time complexity per level, and so we have a time complexity of <i>Θ(n log n)</i>. One of the disadvantages of this algorithm when comparing with the previous algorithm is that we don't sort elements in-place, and we create a new sorted array instead, having a memory complexity of <i>Θ(n)</i>. </p>
<div>
<p align="center">
  <img height="40%" "="" width="40%" src="./README.md_files/merge.png">
</p>
</div>
<p><strong>Challenge</strong>: Implement merge sort. </p>
<ul>
<li>[ ] <code>mergeSort(T [] array)</code></li>
</ul>
<h5 id="heapsort">Heap Sort</h5>
<p>We now introduce a new algorithm called <strong><em>heap sort</em></strong>, which sorts elements <strong><em>in-place</em></strong> and still maintains a running time of <i>O(n log n)</i>. Hence, this sorting technique combines the better attributes of the sorting algorithms that were previously studied. The term <strong><em>heap</em></strong> was originally coined in the context of heap sort, but nowadays is also referred to be <em>garbage-collected storage</em>. We shall refer to this term as the data structure that was initially used in the heap sort. </p>
<p>The <strong><em>heap</em></strong> data structure is an array that we can view as a nearly complete binary tree. Each element in the array correspond to a node in the binary tree. This tree is filled in every level except for the lowest, which is filled from the left up to a point. We can define the <em>heap size</em> to be the size of the heap, which not necessarily is the size of the array, as we're going to verify later. The root of the tree is given by the element with index <strong><em>0</em></strong>. By giving the index <strong><em>i</em></strong> of a node, we can compute its parent, left child and right child. We can define the <strong><em>height</em></strong> of a node to be the longest simple downward path from the node to a leaf.</p>
<div>
<p align="center">
  <img height="100%" "="" width="100%" src="./README.md_files/heap.png">
</p>
</div>
<p><br></p>
<ul>
<li><code>parent(i) = floor((i - 1) / 2)</code></li>
<li><code>right(i) = 2i + 1</code></li>
<li><code>left(i) = 2i + 2</code></li>
</ul>
<h6 id="heap">Heap</h6>
<p>There are two kinds of binary heaps: <strong><em>max heaps</em></strong> and <strong><em>min heaps</em></strong>, where every node must satisfy the <strong><em>heap property</em></strong>: </p>
<div style="border-radius: 25px; border: 2px solid gray;">
<p style="margin-top: 13px; margin-right: 15px; margin-left: 15px;" align="center">
  In a <i>max heap</i> the key of a parent is <span>≥</span> the keys on its children. In a <i>min heap</i> the key of a parent is <span>≤</span> the keys on its children.
</p>
</div>
<p><br></p>
<p>Since a heap of <strong><em>n</em></strong> elements is based on a complete binary tree, its height is <i>Θ(lg n)</i>. Similarly to a balanced binary search tree, basic operations run in <i>O(lg n)</i>. Several basic procedures will be presented, as we will see how they are used in the sorting algorithm and a <em>priority-queue</em> data structure. </p>
<ul>
<li><code>maxHeapify</code>, runs in <em>O(log n)</em> time and is the key to maintain the heap property. </li>
<li><code>buildMaxHeap</code>, runs in <em>O(n)</em> time, produces a <em>max</em> or <em>min heap</em> from an unordered input array.</li>
<li><code>heapsort</code>, runs in <em>O(n log n)</em> time, sorts an array in place.</li>
</ul>
<p>In order to maintain the the <strong><em>heap property</em></strong> we define a procedure <code>maxHeapify</code>, which receives an index <strong><em>i</em></strong> of the array as an argument. This method assumes that trees rooted at the childs of this node are max heaps by themselves, but the <strong><em>i</em></strong>-th element of the array might be smaller <code>A[i]</code> than its children. In that case, <code>A[i]</code> floats down through the heap. At each step, <code>A[i]</code> is compared with both its children, and in case of being smaller than one of them, we perform a swap operation between <code>A[i]</code> and the children with the greatest key. We then repeat this procedure until <code>A[i]</code> is either a leaf, or a root of a <em>max-heap</em>. The figure below shows an example of how <code>maxHeapify</code> would behave if it was running on node <strong><em>i</em></strong>. The running time of <code>maxHeapify</code> on a subtree of size <em>n</em> rooted at a given node <em>i</em> is <em>Θ(1)</em> time to swap elements in case its needed, plus the time to run <code>maxHeapify</code> on one of the children's of node <em>i</em>. The children's subtrees each have size of at most <i>2 n / 3</i>. Therefore we can describe the running time of <code>maxHeapify</code> with a recurrence: <em>T(n) = T(2n / 3) + Θ(1)</em>, which is equivalent to be <em>O(log n)</em> or <em>O(h)</em>.</p>
<div>
<p align="center">
  <img height="90%" "="" width="90%" src="./README.md_files/max-heapify.png">
</p>
</div>
<p>We now can use the <code>maxHeapify</code> method to build a <em>max heap</em> with <code>buildMaxHeap</code> in a bottom-up manner. We know that the set of elements <code>A[n/2 ... n - 1]</code> are all leaves, and therefore the algorithm is pretty straightforward:</p>
<pre><code class="hljs vbnet"><span class="hljs-keyword">from</span> i = n / <span class="hljs-number">2</span> - <span class="hljs-number">1</span> down <span class="hljs-keyword">to</span> <span class="hljs-number">0</span>:
    <span class="hljs-keyword">do</span> maxHeapify(A, i); 
</code></pre>
<p>We are now assuring that trees rooted at the children of each node are <em>max heaps</em>, because we are running <code>maxHeapify</code> on the lower levels first. And therefore we are making sure that each subtree respects the <em>heap property</em>. Each call to <code>maxHeapify</code> runs in <em>O(log n)</em> time, and <code>buildMaxHeap</code> makes <em>O(n)</em> calls to <code>maxHeapify</code>. Hence, the running time of this algorithm is <em>O(n log n)</em>. But this running time is an upper bound which is not asymptotically tight. Our tighter analysis will rely on the fact that the tree has height of order <em>log n</em>, and at most <i>n / 2<sup>h + 1</sup></i> nodes of any height <em>h</em>. Thus, we can express the cost of <code>buildMaxHeap</code> as being bounded by:</p>
<div>
<p align="center">
  <img height="25%" "="" width="25%" src="./README.md_files/buildmaxheap1.png">
</p>
</div>
<p>which is equivalent to,</p>
<div>
<p align="center">
  <img height="25%" "="" width="25%" src="./README.md_files/buildmaxheap2.png">
</p>
</div>
<p>And therefore, we can run build a heap from an unordered array in linear time. The heapsort algorithm starts by using <code>buildMaxHeap</code> to build a <em>max-heap</em> <em>A[0 … n - 1]</em> where <em>n = A.length</em>. Since the maximum element is stored at the root, we can put this element in the final position, exchanging <em>A[0]</em> with <em>A[n - 1]</em>, and consider that the heap has now <em>n - 1</em> elements. We should now restore the <em>max-heap property</em>, in case it is violated, we can call <em>maxHeapify</em> on the root, which leaves a <em>max-heap</em> in <em>A[0 … n - 2]</em>. We now repeat this process for the <em>max-heap</em> of size <strong><em>n - 1</em></strong> down to a heap of size <strong><em>2</em></strong>. This algorithm takes time <i><b>O(n log n)</b></i>, since calling <code>buildMaxHeap</code> takes <i><b>O(n)</b></i> time, and each of the <code>maxHeapify</code> calls takes <i><b>O(log n)</b></i>.</p>
<p><strong>Challenge</strong>: Implement heap sort. </p>
<ul>
<li>[ ] <code>maxHeapify(T [] array, int index, int heapSize)</code></li>
<li>[ ] <code>buildMaxHeap(T [] array, int index, int heapSize)</code></li>
<li>[ ] <code>heapSort(T [] array)</code></li>
</ul>
<h5 id="quicksort">Quick Sort</h5>
<p>Even though that this algorithm has a worst case running time of <i>Θ(n<sup>2</sup>)</i>, quicksort is often the best pratical choice for sorting because it is remarkably efficient on average. Its expected running time is <i>Θ(n log n)</i> and the constant factors hidden in the <i>Θ(n log n)</i> are quite small. With <strong><em>quicksort</em></strong>, we're applying a <em>divide-and-conquer</em> process for sorting sub-arrays. </p>
<ul>
<li><p><strong>Divide</strong>: We partition the main array <em>A[p … r]</em> into two sub-arrays such that <em>A[p … q - 1]</em> contains elements less or equal to <em>A[q]</em> and <em>A[q + 1 … r]</em> contains elements greater or equal than <em>A[q]</em>. Compute index <em>q</em> as part of this step. </p></li>
<li><p><strong>Conquer</strong>: Sort the sub-arrays <em>A[p … q - 1]</em> and <em>A[q + 1 … r]</em> with recursive calls.</p></li>
<li><p><strong>Combine</strong>: Since the sub-arrays are sorted, no work is needed to combine them, the entire array <em>A[p .. r]</em> is now sorted.</p></li>
</ul>
<p>We first consider the case when we always choose <em>A[n - 1]</em> as pivot. We can then arrange the array <strong><em>in-place</em></strong>, as explained above. The following image can exemplify the general approach to arrange a given array, forming the sub-array of smaller elements (in red), and the sub-array of greater elements (in blue).</p>
<div>
<p align="center">
  <img height="100%" "="" width="100%" src="./README.md_files/quicksort.png">
</p>
</div>
<p>The running time of <strong><em>quicksort</em></strong> depends on whether the partitioning is balanced or unbalanced, which in turn depends on which elements we choose as pivots. </p>
<p>Therefore, the <strong><em>worst-case behaviour</em></strong> for quicksort occurs when we obtain one sub-partition with <em>n - 1</em> elements and one with <em>0</em> elements. We can represent the recurrence for the running time as:</p>
<p><i>T(n) = T(n - 1) + T(0) + Θ(n)</i></p>
<p>which intuitively evaluates to <i>Θ(n<sup>2</sup>)</i>. Therefore, if the partitioning is maximally unbalanced at every level, we have a running time of <i>Θ(n<sup>2</sup>)</i>. </p>
<p>The <strong><em>best-case partitioning</em></strong> occurs in the most even possible split. If each of the partitions are of size <em>n / 2</em>, <strong><em>quicksort</em></strong> runs much faster. The recurrence for the running time is:</p>
<p><i>T(n) = 2 × T(n / 2) + Θ(n)</i></p>
<p>which can be translated into <i>Θ(n log n)</i>. Hence, by equally balancing the two sides of the algorithm at every level, we get an asymptotically faster algorithm. </p>
<p>The <strong><em>average-case</em></strong> running time of quicksort is much closer to the best case than to the worst case. If we have a recurrence of:</p>
<p><i>T(n) = T(9 n / 10) + T(n / 10) + cn</i></p>
<p>we would still reach a <strong>logarithmic</strong> depth of <i>log<sub>10</sub>n = Θ(log n)</i>. The total cost of quicksort would still be <i>O(n log n)</i>. </p>
<p>However, we can add some randomization to this algorithm by choosing a <strong><em>random pivot</em></strong>, and expect to obtain good performance over all inputs. By following a technique called <strong><em>random sampling</em></strong> we can randomly select an element from the array to be the pivot. Because we randomly choose the pivot, we expect to split the array reasonably well. </p>
<p><strong>Challenge</strong>: Implement quicksort recursively and iteratively.</p>
<ul>
<li>[ ] <code>quicksort(T [] array)</code></li>
</ul>
<h5 id="sortinginlineartime">Sorting in linear time</h5>
<p>The previous algorithms were capable of sorting a set of <strong><em>n</em></strong> numbers in <em>O(n log n)</em>, with <em>merge sort</em> and <em>heap sort</em> achieving this upper bound in the worst case scenario. The sorted order that these algorithms produce is based on comparisons between the input elements, and thus we call them <strong><em>comparison sorts</em></strong>. We shall prove that any comparison sort must make <i><b>Ω(n log n)</b></i> comparisons in the worst case to sort <em>n</em> elements. Thus, <em>merge sort</em> and <em>heap sort</em> are asymptotically optimal, and no comparison sort exist that is faster by more than a constant factor. We are also going to examine three more algorithms that sort in linear time, <strong><em>counting sort</em></strong>, <strong><em>radix sort</em></strong>, and <strong><em>bucket sort</em></strong>. </p>
<p><br></p>
<p>In a comparison sort we only use comparisons between elements in order to sort a sequence. Therefore, given two elements, <i>a<sub>i</sub></i> and <i>a<sub>j</sub></i>, we perform one of the following comparisons: <span><i>a<sub>i</sub></i> = <i>a<sub>j</sub></i>, <i>a<sub>i</sub></i> &lt; <i>a<sub>j</sub></i>, <i>a<sub>i</sub></i> &gt; <i>a<sub>j</sub></i>, <i>a<sub>i</sub></i> ≤ <i>a<sub>j</sub></i>, <i>a<sub>i</sub></i> ≥ <i>a<sub>j</sub></i></span> per step. We can view comparison sorts in terms of <strong><em>decision trees</em></strong>. The image bellow shows a map between concepts.</p>
<div>
<p align="center">
  <img height="70%" "="" width="70%" src="./README.md_files/map.png">
</p>
</div>
<p>We shall prove that any comparison sort algorithm requires <i><b>Ω(n log n)</b></i> comparisons in the worst case, by stating that decisions trees are binary, and the number of leafs is at least as big as the number of possible answers, <i>n!</i>. Since a binary tree of height <i>h</i> has no more than <span>2<sup>h</sup></span> leaves, we have:</p>
<p><i>n! ≤ l ≤ 2<sup>h</sup></i>,</p>
<p>which by taking logarithms, implies</p>
<p><i>h ≥ lg(n!) = Ω(n log n)</i></p>
<p><strong><em>Counting Sort</em></strong> assumes that each of the input elements is an integer in the range <strong><em>0 to k</em></strong>. This algorithm determines, for each input element <strong>x</strong>, the number of elements less than <strong>x</strong>. It uses this information to place this element directly into the result array. If 17 elements are less than <strong>x</strong>, then <strong>x</strong> belongs in output position 17. You can find this algorithm written in Java bellow,</p>
<pre><code class="Java language-Java hljs nimrod">public <span class="hljs-type">Integer</span> [] countingSort(<span class="hljs-type">Integer</span> [] <span class="hljs-type">array</span>) {
    /* <span class="hljs-type">Find</span> max value */
    <span class="hljs-type">int</span> max = <span class="hljs-number">0</span>;
    <span class="hljs-keyword">for</span> (<span class="hljs-type">Integer</span> integer : <span class="hljs-type">array</span>) {
        <span class="hljs-keyword">if</span> (integer &lt; <span class="hljs-number">0</span>) {
            throw new <span class="hljs-type">IllegalArgumentException</span>();
        } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (integer &gt; max) {
            max = integer;
        }
    }
    <span class="hljs-type">Integer</span> [] <span class="hljs-literal">result</span> = new <span class="hljs-type">Integer</span>[<span class="hljs-type">array</span>.length], // <span class="hljs-type">The</span> <span class="hljs-literal">result</span> <span class="hljs-type">array</span>
        aux = new <span class="hljs-type">Integer</span>[max + <span class="hljs-number">1</span>]; // <span class="hljs-type">The</span> auxiliary <span class="hljs-type">array</span> <span class="hljs-keyword">is</span> filled <span class="hljs-keyword">with</span> <span class="hljs-number">0</span>'s following <span class="hljs-type">Java</span>'s language specs
    <span class="hljs-keyword">for</span> (<span class="hljs-type">Integer</span> element : <span class="hljs-type">array</span>) {
        aux[element] = <span class="hljs-number">1</span>;
    }
    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt; aux.length; ++i) { // aux[i] contains the number <span class="hljs-keyword">of</span> elements less <span class="hljs-keyword">or</span> equal than i
        aux[i] += aux[i - <span class="hljs-number">1</span>];
    }
    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-type">array</span>.length - <span class="hljs-number">1</span>; i &gt;= <span class="hljs-number">0</span>; --i) {
        <span class="hljs-literal">result</span>[aux[<span class="hljs-type">array</span>[i]]] = <span class="hljs-type">array</span>[i]; // placing the element <span class="hljs-keyword">in</span> the correct / sorted index
        aux[<span class="hljs-type">array</span>[i]]--; // <span class="hljs-type">Decrement</span> number <span class="hljs-keyword">of</span> elements before a certain position (useful <span class="hljs-keyword">if</span> we have repeated numbers)
    }
    <span class="hljs-keyword">return</span> <span class="hljs-literal">result</span>;
}
</code></pre>
<p>This algorithm beats the lower bound of <i><b>Ω(n log n)</b></i> because it's not a comparison sort. <em>Counting sort</em> uses the actual values of the elements to index into an array. Another important property of counting sort is that is stable. Numbers with the same value appear in the output array in the same order as they do in the input array. The property of stability is important when we carry satellite data around with the element being sorted. </p>
<p><strong><em>Radix Sort</em></strong> uses counting sort as a subroutine. It recursively sorts numbers on their <em>least significant</em> digits first. The process includes to sort number on all <strong><em>d</em></strong> digits, and remarkably, at that point, the numbers are fully sorted. Hence, we have to perform <strong><em>d</em></strong> passes through the entire array in order to sort it. </p>
<div>
<p align="center">
  <img height="60%" "="" width="60%" src="./README.md_files/radix.png">
</p>
</div>
<p>The analysis of the running time depends on the algorithm used to sort each digit, which has to be stable. When <strong><em>d</em></strong> is constant and k = O(n), we can make radix sort run in linear time. We can, for instance, sort by  <strong><em>bit</em></strong>. Each pass over <strong><em>n</em></strong> <strong><em>d-digits</em></strong> numbers take <i>Θ(n + k)</i>, and since we have <strong><em>d</em></strong> passes, the total time for radix sort is <i>Θ(d(n + k))</i>.</p>
<h4 id="dynamicprogramming">Dynamic Programming</h4>
<p><strong><em>Dynamic Programming</em></strong> is one of my favourites algorithms modules. We can compare it to divide-and-conquer methods that solve problems by combining solutions of sub problems. We apply dynamic programming when sub problems overlap. In this context, divide-and-conquer methods do more work than necessary, solving every sub problem even if it was already solved before. A method whose approach is <strong><em>Dynamic Programming</em></strong> solves every sub-problem at most once, saving the result and avoiding to recompute the answer every time it solves each sub-problem. We can typically apply dynamic programming to <strong><em>optimization problems</em></strong> that can have many possible solutions. Each solution has a value and we wish to find the optimal value (minimum or maximum). We can follow a sequence of steps in order to build a dynamic programming algorithm,</p>
<ol>
<li>Characterize the structure of an optimal solution.</li>
<li>Recursively define the value of an optimal solution</li>
<li>Compute the value of an optimal solution, typically in a bottom-up fashion.</li>
<li>Construct an optimal solution from computed information.</li>
</ol>
<p>We know that we can solve problems of size <em>n</em>, by solving smaller problems of the same type, but of smaller sizes. The overall optimal solution can incorporate optimal solutions from sub problems, and therefore we say that this problem has an <strong><em>optimal substructure</em></strong>. We can use <strong><em>dynamic programming</em></strong> to solve this problem efficiently by saving sub problems solutions, and reuse them when these sub problems overlap. This method uses additional memory, it serves an example of a <strong><em>time-memory trade-off</em></strong>. Hence, we can transform an exponential-time solution into a polynomial-time solution. A <strong><em>dynamic programming</em></strong> approach runs in polynomial time, when the number of distinct sub problems to be solved is polynomial. 
We usually have two equivalent ways to implement a dynamic programming approach. The first is called <strong><em>top-down with memoization</em></strong>, where we write the procedure recursively in a natural manner, but modified to save the result of each sub-problem (in an array or hash table). The procedure can then check whether the sub problem that is trying to solve was already solved or not, and thus return the saved value if it was. Therefore, the solution to a given sub problem is <strong><em>memoized</em></strong>, as this procedure "remembers" what results it computed previously. The second approach is the <strong><em>bottom-up method</em></strong>, where we sort the sub problems by size, and solve them in size order, smallest first. When solving larger problems, we've already solved all of the smaller sub problems its solution depends on. </p>
<h5 id="matrixchainmultiplication">Matrix Chain Multiplication</h5>
<p>The Matrix Chain Multiplication problem is defined by the product of several matrices. Given a chain of <strong><em>n</em></strong> matrices <i>{ A<sub>1</sub>, A<sub>2</sub>, A<sub>3</sub>, A<sub>4</sub> …. A<sub>n - 1</sub> }</i> we wish to compute the product of the matrices, following the order in the original chain. Matrix multiplication is associative, hence all multiplications result in the same product. Given a chain of <i>{ A<sub>1</sub>, A<sub>2</sub>, A<sub>3</sub>, A<sub>4</sub> }</i> of four matrices, we can <strong><em>fully parenthesize</em></strong> the product in five different ways. </p>
<ul>
<li><i>( A<sub>1</sub> ( A<sub>2</sub>, ( A<sub>3</sub> A<sub>4</sub> ))) </i></li>
<li><i>( A<sub>1</sub> (( A<sub>2</sub> A<sub>3</sub> ) A<sub>4</sub> )) </i></li>
<li><i>(( A<sub>1</sub>  A<sub>2</sub> )( A<sub>3</sub> A<sub>4</sub> )) </i></li>
<li><i>(( A<sub>1</sub> ( A<sub>2</sub> A<sub>3</sub> )) A<sub>4</sub> ) </i></li>
<li><i>((( A<sub>1</sub>  A<sub>2</sub> ) A<sub>3</sub> ) A<sub>4</sub> ) </i></li>
</ul>
<p>How we parenthesize this chain can have a big impact on the cost of performing the product. Certain multiplications result in smaller matrices, and therefore result in a smaller use of computational resources. In order to multiply tow matrices <strong><em>A</em></strong> and <strong><em>B</em></strong>, they have to be compatible: the number of columns of <strong><em>A</em></strong> has to be equal to the number of rows of <strong><em>B</em></strong>. If <strong><em>A</em></strong> is a <i>p × q</i> matrix and <strong><em>B</em></strong> is a <i>q × r </i> matrix, then the resulting matrix <strong><em>C</em></strong> is of size <i>p × r</i>. The time to compute <strong><em>C</em></strong> is therefore <i>p × q × r</i>, which corresponds to the number of scalar multiplications to calculate a matrix. Hence we want to <strong><em>minimize the number of scalar multiplications</em></strong>. We can define a fully parenthesized matrix product to be the product of two fully parenthesized matrix sub products, and the split between the two sub products may occur anywhere between <i>k = 1, 2, …. n - 1</i>. Thus, we obtain the recurrence:</p>
<p><i>P(n) = P(k) . P(n - k)</i></p>
<p>This recurrence means that for every sub sequence with <i>n ≥ 2</i> we produce two more sub problems, and therefore the recurrence is <i>Ω(n<sup>2</sup>)</i>, which is exponential. Hence, the brute force strategy is poor. Because this problem has an <strong><em>Optimal Substructure</em></strong> we can solve all sub problems the same way we would solve the original problem, and thus we have,</p>
<pre><code class="Python language-Python hljs stylus"><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range (<span class="hljs-tag">i</span> + <span class="hljs-number">1</span> .. j):
  <span class="hljs-function"><span class="hljs-title">DP</span><span class="hljs-params">(i, j)</span></span> = <span class="hljs-function"><span class="hljs-title">min</span><span class="hljs-params">(DP(i, k)</span></span> + <span class="hljs-function"><span class="hljs-title">DP</span><span class="hljs-params">(k + <span class="hljs-number">1</span>, j)</span></span> + <span class="hljs-function"><span class="hljs-title">costOf</span><span class="hljs-params">(A(i, k)</span></span> . <span class="hljs-function"><span class="hljs-title">B</span><span class="hljs-params">(k, j)</span></span>)) 
</code></pre>
<p>Observe that the number of overlapping sub problems is large, and therefore we have relatively few distinct sub problems.  We can see how sub problems overlap in the image bellow, which takes as an example the same chain of 4 matrices. This property of overlapping sub problems is the second hallmark of when dynamic programming applies (the first hallmark being optimal substructure). Whenever a problem exhibits optimal substructure, we have a good clue that dynamic programming might apply.</p>
<div>
<p align="center">
  <img height="70%" "="" width="70%" src="./README.md_files/overlaps.png">
</p>
</div>
<h5 id="longestcommonsubsequence">Longest Common Sub-sequence</h5>
<p>A sub sequence of a given sequence is the given sequence with zero or more elements left out. Given a sequence <i>X = { x<sub>1</sub>, x<sub>2</sub> … x<sub>m</sub> }</i> and <i>Y = { y<sub>1</sub>, y<sub>2</sub> … y<sub>k</sub> }</i>, we say that a common subsequence exists if exists a strictly increasing sequence of indices where <i>x<sub>i</sub> = y<sub>j</sub></i>. For example, if <i>X = { A, B, C, B, D, A, B }</i> and <i>Y = { B, D, C, A, B, A }</i>, the sequence <i>{ B, C, A }</i> is a <strong><em>common sub sequence</em></strong>. In the  <strong><em>longest common sub sequence problem</em></strong>, we are given two sequences and wish to find a maximum-length common sub sequence of <em>X</em> and <em>Y</em>. </p>
<p>In a brute-force approach, we would enumerate all sub sequences of <em>X</em> and check if it is also a sub sequence of <em>Y</em>, keeping track of the longest sub sequence we could find. Because <em>X</em> has <span>2<sup>m</sup></span> sub sequences, this approach requires exponential time. But lets find an <strong><em>optimal sub structure</em></strong> in this problem, with <i>Z = { z<sub>1</sub>, z<sub>2</sub> … z<sub>k</sub> }</i> being any common sub sequence of <em>X</em> and <em>Y</em>. </p>
<ol>
<li>If <i>x<sub>m</sub> = y<sub>n</sub></i>, then <i>z<sub>k</sub> = x<sub>m</sub> = y<sub>n</sub></i> and <i>Z<sub>k - 1</sub></i> is an LCS of <i>X<sub>m - 1</sub></i> and <i>Y<sub>n - 1</sub></i>.</li>
<li>If <i>x<sub>m</sub> ≠ y<sub>n</sub></i>, then <i>z<sub>k</sub> ≠ x<sub>m</sub></i> implies that <i>Z</i> is an LCS of <i>X<sub>m - 1</sub></i> and <i>Y</i>.</li>
<li>If <i>x<sub>m</sub> ≠ y<sub>n</sub></i>, then <i>z<sub>k</sub> ≠ y<sub>n</sub></i> implies that <i>Z</i> is an LCS of <i>X</i> and <i>Y<sub>n - 1</sub></i>.</li>
</ol>
<p>Let us now define a recurrence to this problem. We can first declare <i>length[i, j]</i> to be the length of a common sequence of <i><b>X<sub>i</sub></b></i> and <i><b>Y<sub>j</sub></b></i>. If <i>i = 0</i> or <i>j = 0</i>, one of the sequences has length 0, and so the <strong><em>common sub sequence</em></strong> would have length 0.  </p>
<div>
<p align="center">
  <img height="80%" "="" width="80%" src="./README.md_files/lcs.png">
</p>
</div>
<p>The following snippet of code solves this problem in <i>Θ(nm)</i> time, using a memory complexity of <i>Θ(nm)</i> as well. Even though that this procedure returns the length of the longest common sub sequence, we could easily find what are the elements of that sequence maintaining the same asymptotic factors. </p>
<pre><code class="Java language-Java hljs glsl">public <span class="hljs-keyword">int</span> longestCommonSubsequence(T [] x, T [] y) {
    Integer [][] <span class="hljs-built_in">length</span> = new Integer[x.<span class="hljs-built_in">length</span>][y.<span class="hljs-built_in">length</span>];
    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; x.<span class="hljs-built_in">length</span>; ++i) {
        <span class="hljs-built_in">length</span>[i][<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>;
    }
    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; y.<span class="hljs-built_in">length</span>; ++j) {
        <span class="hljs-built_in">length</span>[<span class="hljs-number">0</span>][j] = <span class="hljs-number">0</span>;
    }
    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt; x.<span class="hljs-built_in">length</span>; ++i) {
        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; y.<span class="hljs-built_in">length</span>; ++j) {
            <span class="hljs-keyword">if</span> (x[i].equals(y[j])) {
                <span class="hljs-built_in">length</span>[i][j] = <span class="hljs-built_in">length</span>[i - <span class="hljs-number">1</span>][j - <span class="hljs-number">1</span>] + <span class="hljs-number">1</span>;
            } <span class="hljs-keyword">else</span> {
                <span class="hljs-built_in">length</span>[i][j] = Math.<span class="hljs-built_in">max</span>(<span class="hljs-built_in">length</span>[i - <span class="hljs-number">1</span>][j], <span class="hljs-built_in">length</span>[i][j - <span class="hljs-number">1</span>]);
            }
        }
    }
    <span class="hljs-keyword">return</span> <span class="hljs-built_in">length</span>[x.<span class="hljs-built_in">length</span> - <span class="hljs-number">1</span>][y.<span class="hljs-built_in">length</span> - <span class="hljs-number">1</span>];
}
</code></pre>
<p>The following table can represent the final state of <code>length</code>, including the longest common subsequence of both sequences. </p>
<div>
<p align="center">
  <img height="40%" "="" width="40%" src="./README.md_files/lcs-table.png">
</p>
</div>
<h5 id="optimalbinarysearchtrees">Optimal binary search trees</h5>
<p>We need an <strong><em>Optimal Binary Search Tree</em></strong> whenever we want to design a program to translate sentences or find synonyms of words. Because we will search the tree for each individual word in the text, we want the total time spent searching to be as low as possible. We also want to place frequent words nearer to the root, in order to "reward" their lookups. We are given a sequence <i>K = { k<sub>1</sub>, k<sub>2</sub>, k<sub>3</sub> … k<sub>n</sub> } </i> of <em>n</em> distinct keys in sorted order, and we wish to build a binary search tree from these keys. For each <i>k<sub>i</sub></i>, we have probability <i>p<sub>i</sub></i> that a search will occur for <i>k<sub>i</sub></i>. Some searches may occur for keys that are not in <i>K</i>, hence we have to be capable of handling such cases. Therefore, we have that,</p>
<div>
<p align="center">
  <img height="16%" "="" width="16%" src="./README.md_files/probabilities.png">
</p>
</div>
<p>and we can define the search cost of the binary search tree as,</p>
<div>
<p align="center">
  <img height="55%" "="" width="55%" src="./README.md_files/search-cost.png">
</p>
</div>
<p>Therefore, given a set of keys and probabilities, we wish to construct a binary search tree whose expected search cost is smallest. We call this tree an <strong><em>optimal binary search tree</em></strong>. This condition doesn't mean that we have to construct a tree whose overall height is smallest. Nor that we have to place the key with highest probability on the root. We want to exhaustive check all the possibilities, and because we would have to examine an exponential number of binary search trees, it is difficult to come up with an efficient algorithm. Not surprisingly, we shall solve this problem with <strong><em>dynamic programming</em></strong>. </p>
<p>In order to <em>"maintain"</em> the structure of a binary search tree, we shall keep our keys in a sorted array with <em>n</em> elements. We should then search for the optimal root with index <i>k<sub>r</sub></i> <span>∈</span> <i>{ k<sub>i</sub> … k<sub>j</sub> }</i> (between index <em>i</em> and index <em>j</em>). Notice that in the first iteration <em>i = 0</em> and <em>j = n - 1</em>. Because we know that the left sub tree must be rooted between <i>{ k<sub>i</sub> … k<sub>r - 1</sub> }</i> we can repeat the same procedure of finding the next root between that set of elements. Keep in mind that we can execute the same procedure in the right sub tree,  <i>{ k<sub>r + 1</sub> … k<sub>j</sub> }</i>.  Because we know that we are trying to obtain the optimal solution in each level, we also know that our problem has an optimal substructure. We also know that some sub problems will overlap, and therefore we can memorize its solution so that we don't have to solve them again. </p>
<p><strong><em>Problem:</em></strong> Given a set of words, and its number of appearances in a sufficiently big sample text, we want to build an optimal binary search tree:</p>
<pre><code class="Java language-Java hljs ini"><span class="hljs-setting">keys = <span class="hljs-value">{ <span class="hljs-string">"rate"</span>, <span class="hljs-string">"percentage"</span>, <span class="hljs-string">"fatality"</span>, <span class="hljs-string">"casualty"</span>, <span class="hljs-string">"quotas"</span> }</span></span>
<span class="hljs-setting">appearances = <span class="hljs-value">{ <span class="hljs-number">5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span> }</span></span>
</code></pre>
<p>We can first define a <strong><em>Node</em></strong> that will keep information structured,</p>
<pre><code class="Java language-Java hljs processing">class Node&lt;T&gt; {
    T <span class="hljs-variable">key</span>;
    <span class="hljs-built_in">int</span> appearances;
    Node(T <span class="hljs-variable">key</span>, <span class="hljs-built_in">int</span> appearances) {
        <span class="hljs-keyword">if</span> (<span class="hljs-keyword">this</span>.appearances &lt;= <span class="hljs-number">0</span>) {
            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> IllegalArgumentException();
        }
        <span class="hljs-keyword">this</span>.<span class="hljs-variable">key</span> = <span class="hljs-variable">key</span>;
        <span class="hljs-keyword">this</span>.appearances = appearances;
    }
    @Override
    <span class="hljs-keyword">public</span> <span class="hljs-built_in">int</span> compareTo(Node node) {
        <span class="hljs-keyword">return</span> <span class="hljs-variable">key</span>.compareTo(node.<span class="hljs-variable">key</span>);
    }
}
</code></pre>
<p>Hence, we create a new array of nodes, making it easier to sort these nodes by key, and keep every number of appearances along. The next step is to create two auxiliary matrices that will keep track of every optimal cost, and optimal root from <strong><em>i</em></strong> to <strong><em>j</em></strong>.</p>
<pre><code class="Java language-Java hljs markdown">costs = new Double[<span class="hljs-link_label">nodes.length</span>][<span class="hljs-link_reference">nodes.length</span>];
roots = new Integer[<span class="hljs-link_label">nodes.length</span>][<span class="hljs-link_reference">nodes.length</span>];
</code></pre>
<p>After initializing the <code>costs</code> matrix elements with <span>+∞</span> and <code>roots</code> matrix elements with an <code>UNDEFINED</code> flag, we can build our tree with <strong><em>dynamic programming</em></strong>,</p>
<pre><code class="Java language-Java hljs cpp"><span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-keyword">double</span> <span class="hljs-title">buildTree</span><span class="hljs-params">(<span class="hljs-keyword">int</span> i, <span class="hljs-keyword">int</span> j, <span class="hljs-keyword">int</span> level)</span> </span>{
    <span class="hljs-keyword">if</span> (roots[i][j] != UNDEFINED) {
        <span class="hljs-keyword">return</span> costs[i][j];
    } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (i == j) {
        <span class="hljs-keyword">return</span> nodes[i].appearances * Math.<span class="hljs-built_in">log</span>(level);
    }
    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> r = i; r &lt;= j; r++) {
        <span class="hljs-keyword">double</span> candidateCost = buildTree(i, r - <span class="hljs-number">1</span>, level + <span class="hljs-number">1</span>) +
                buildTree(r + <span class="hljs-number">1</span>, j, level + <span class="hljs-number">1</span>) + nodes[r].appearances * Math.<span class="hljs-built_in">log</span>(level);
        <span class="hljs-keyword">if</span> (candidateCost &lt; costs[i][j]) {
            costs[i][j] = candidateCost;
            roots[i][j] = r;
        }
    }
    <span class="hljs-keyword">return</span> costs[i][j];
}
</code></pre>
<p>At this point it is obvious that the index of the root of our tree is at <code>r = roots[0][n - 1]</code>, and the root of its left sub tree is at <code>roots[0][r - 1]</code>. Similarly the root of its right sub tree is at <code>roots[r + 1][n - 1]</code>. That being said, it is now easy to find the next sub trees. <strong><em>Search operations</em></strong> can now be made with a similar strategy to the one found in binary search trees, comparing only keys. These searches have a time complexity of <i><b>O(log h)</b></i> with <strong><em>h</em></strong> being the height of the new optimal binary search tree.</p>
<p><strong>Challenge</strong>: Implement a method (following three approaches <em>recursive</em>, <em>top-down</em> and <em>bottom-up</em>) that returns the i-<i>th</i> number of the Fibonacci sequence. The first strategy must not use dynamic programming. Solve the <strong><em>string edit distance</em></strong> problem by using the same three strategies.</p>
<ul>
<li>[ ] <code>recursiveFib(int i)</code></li>
<li>[ ] <code>topDownFib(int i)</code></li>
<li>[ ] <code>bottomUpFib(int i)</code></li>
<li>[ ] <code>recursiveEditDist(String a, String b)</code></li>
<li>[ ] <code>topDownEditDist(String a, String b)</code></li>
<li>[ ] <code>bottomUpEditDist(String a, String b)</code></li>
</ul>
<h4 id="graphalgorithms">Graph Algorithms</h4>
<h5 id="minimumspanningtrees">Minimum Spanning Trees</h5>
<p>Let us have a connected and undirected graph <i>G = (V, E)</i>. For each edge <i>(u, v) ∈ E</i>, we have a weight <i>w(u, v)</i>. We then wish to find an acyclic <strong>subset</strong> that connects all of the vertices and whose total weight <i><b>w(T) = ∑ w(u, v)</b></i> is minimized. We shall examine two algorithms to solve this problem: <strong><em>Kruskal's and Prim's</em></strong>. These algorithms can be easily implemented by using <em>binary heaps</em>, but their run-time complexity can be improved when using <strong><em>fibonacci heaps</em></strong>. The two algorithms are greedy, making the choice that is best at each moment. </p>
<p>The generic algorithm to solve this problem consists on growing an edge <i>(u, v)</i> that we can add to <i>A</i> without violating the invariant, in the sense that <i>A ∪ { (u, v) }</i> is also a subset of a minimum spanning tree. Such edge is called <strong><em>safe edge</em></strong>, since it maintains the invariant. </p>
<pre><code class="hljs stata">MinSpanningTree(<span class="hljs-keyword">G</span>(V, <span class="hljs-keyword">E</span>, W)):
  A = {}
  <span class="hljs-keyword">while</span> A is not a spanning tree:
    find a safe edge (<span class="hljs-keyword">u</span>, v) <span class="hljs-keyword">for</span> A
    A = A <span class="hljs-keyword">U</span> { (<span class="hljs-keyword">u</span>, v) }
  <span class="hljs-keyword">return</span> A
</code></pre>
<p>The process that varies is, of course, the process of finding a <strong><em>safe edge</em></strong>. Let us first define some concepts: A <strong><em>cut</em></strong> (S, V - S) of an undirected graph <i>G = (V, E)</i> is a partition of <i><b>V</b></i>. We say that an edge <i>(u, v) ∈ E</i> <strong><em>crosses</em></strong> the cut <i>(S, V - S)</i> if one of its endpoints is in <i>S</i> and the other is <i>(V - S)</i>. We say that a cut <strong><em>respects</em></strong> a set <i>A</i> of edges if no edge <i>A</i> crosses the cut. An edge is a <strong><em>light edge</em></strong> crossing a cut if its weight is the minimum of any edge crossing the cut. </p>
<h6 id="kruskalsalgorithm">Kruskal's algorithm</h6>
<p>This algorithm finds a safe edge to add to a given growing forest by finding, of all edges, an edge <i>(u, v)</i> of least weight. </p>
<pre><code class="hljs stata">Kruskal(<span class="hljs-keyword">G</span>(V, <span class="hljs-keyword">E</span>, W)):
  A = {}
  <span class="hljs-keyword">sort</span> edges into non-decreasing <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> weight
  <span class="hljs-keyword">for</span> each edge (<span class="hljs-keyword">u</span>, v) previously sorted:
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">u</span> and v are not <span class="hljs-keyword">in</span> the same subtree:
      match <span class="hljs-keyword">u</span> and v
      A = A <span class="hljs-keyword">U</span> { (<span class="hljs-keyword">u</span>, v) }
</code></pre>
<div>
<p align="center">
  <img height="100%" "="" width="100%" src="./README.md_files/kruskal.png">
</p>
</div>
<p>The running time of this algorithm depends on how we implement each data structure, but we can do it in <i><b>O(E log V)</b></i> time. </p>
<h6 id="primsalgorithm">Prim's algorithm</h6>
<p>This algorithm is very similar to the Dijkstra algorithm for finding shortest paths in a graph. It has the property that the edges on set <strong><em>A</em></strong> always form a single tree. Each step it adds a light edge that connects <strong><em>A</em></strong> to a given vertex <i>v ∉ A</i>.  </p>
<pre><code class="hljs stata">Prim(<span class="hljs-keyword">G</span>(V, <span class="hljs-keyword">E</span>, W), root):
  <span class="hljs-keyword">for</span> each v <span class="hljs-keyword">in</span> V:
    v.key = ∞
    v.π = -1
  root.key = 0
  Q = V
  <span class="hljs-keyword">while</span> Q != {}:
    <span class="hljs-keyword">u</span> = extractMin(Q)
    <span class="hljs-keyword">for</span> each v <span class="hljs-keyword">in</span> AdjList(<span class="hljs-keyword">u</span>):
      <span class="hljs-keyword">if</span> v <span class="hljs-keyword">in</span> Q and <span class="hljs-literal">w</span>(<span class="hljs-keyword">u</span>, v) &lt; v.key:
        v.key = <span class="hljs-literal">w</span>(<span class="hljs-keyword">u</span>, v)
        v.π = <span class="hljs-keyword">u</span>
</code></pre>
<div>
<p align="center">
  <img height="100%" "="" width="100%" src="./README.md_files/prim.png">
</p>
</div>
<p>Using a fibonacci heap, we can make this algorithm time complexity <i><b>O(E  + V log V)</b></i>.</p>
<h5 id="shortestpaths">Shortest Paths</h5>
<p>In a single source shortest path problem, we are given a weighted, directed graph <em>G = (V, E)</em>, with a weight function  <i>w : E ∈ R</i>, mapping edges to real-valued weights. The <strong><em>weight</em></strong> <em>w(p)</em> of a path <i>P = {v<sub>0</sub>, v<sub>1</sub>, …. , v<sub>k</sub>}</i> is given as the sum of weights of its edges.</p>
<div>
<p align="center">
  <img height="30%" "="" width="30%" src="./README.md_files/shortest-path.png">
</p>
</div>
<p>We define the <strong>shortest-path</strong> <i> δ(u, v) </i> from <em>u</em> to <em>v</em> by,</p>
<div>
<p align="center">
  <img height="70%" "="" width="70%" src="./README.md_files/shortest-path-def.png">
</p>
</div>
<p>We shall first focus on the <strong>single-source shortest-path problem</strong>. Given a graph <i>G = (V, E)</i>, we want to find a shortest path from a given source vertex. </p>
<p>(I'll continue this later, we have more important things to do!)</p>
<h5 id="maximumflows">Maximum Flows</h5>
<h4 id="karatsubaalgorithm">Karatsuba Algorithm</h4>
<p>Karatsuba discovered an algorithm to calculate multiply two numbers in linear time. The traditional multiplication algorithm takes <i>Θ(n<sup>2</sup>)</i> time. </p>
<div>
<p align="center">
  <img src="./README.md_files/quadratic.png">
</p>
</div>
<p>Karatsuba defines <strong><em>x</em></strong> and <strong><em>y</em></strong> (numbers to be multiplied) by splitting them into 2 equal halfs:</p>
<ul>
<li><i>x = x<sub>1</sub> × r<sup>n/2</sup> + x<sub>0</sub></i></li>
<li><i>y = y<sub>1</sub> × r<sup>n/2</sup> + y<sub>0</sub></i></li>
</ul>
<p>With <strong><em>r</em></strong> denoting the radix of the number representation, and <strong><em>n</em></strong> the number of digits used to represent the number (digits in a word), <b>x<sub>1</sub></b> represents the higher half of <b>x</b> and <b>x<sub>0</sub></b> represents the lower half of <b>x</b>. Similarly, <b>y<sub>1</sub></b> represents the higher half of <b>y</b> and <b>y<sub>0</sub></b> represents the lower half of <b>y</b>. We could obtain the multiplication result by defining <strong><em>z</em></strong>:</p>
<div>
<p align="center">
  let <b>z<sub>0</sub></b> = x<sub>0</sub> × y<sub>0</sub>, <b>z<sub>2</sub></b> = x<sub>1</sub> × y<sub>1</sub>,  <b>z<sub>1</sub></b> = (x<sub>0</sub> × y<sub>1</sub>) × (x<sub>1</sub> × y<sub>0</sub>)
  <br>
  z = x × y = z<sub>0</sub> + z<sub>1</sub> + z<sub>2</sub>
</p>
</div>
<h4 id="npcompleteness">NP-Completeness</h4>
<p>We can start by formalizing our notion of polynomial-time solvable problems. First we assume that if a problem requires time <i>Θ(n<sup>100</sup>)</i>, it is likely that a more efficient algorithm is discovered. Experience has shown that once the first polynomial-time algorithm for a problem has been discovered, more efficient algorithms often follow. Second, for any reasonable computational problem, if a problem can be solved in polynomial time, then it can be solved in polynomial time for any other model. Third, polynomial problems have nice closure properties, since polynomials are closed under adition, multiplication and composition. For example, if the output of one polynomial time was fed into the input of a different polynomial time, the overall resulting algorithm would still be polynomial.  </p>
<p>To understand the class of a <em>polynomial-time problem</em> we must first define what a <em>problem</em> is. We define an <strong><em>abstract problem</em></strong> Q to be the binary relation on a set <strong><em>I</em></strong> of problem <strong><em>instances</em></strong> and a set <strong><em>S</em></strong> of <strong><em>problem solutions</em></strong>. This formulation is perhaps more general than what we need for our purposes. The theory of NP-completeness restricts attention to <strong><em>decision problems</em></strong>, those having a yes/no solution. In this case, we can view an abstract decision problem as a function that maps the instance set <strong><em>I</em></strong> to the solution set <i>{0, 1}</i>. Many abstract problems are not decision problems, but rather <strong><em>optimization problems</em></strong>, which requires some value to be minimized/maximized. However, we can usually recats an optimization problem as a decision problem that is no harder. </p>
<p>In order for a computer to solve abstract problems, we must represent problem instances in a way that a program understands. An <strong><em>enconding</em></strong> of a set <em>S</em> of abstract objects is a mapping <em>e</em> from <em>S</em> to the set of binary strings. Taking as an example the set of natural numbers <i>ℕ = {0, 1, 2, 3, 4 … }</i> as the strings <i>{ 0, 1, 10, 11, 100 … }</i>. We can encode a compound object as a binary string by combining the representation of its constituent parts. Thus, a computer algorithm that solves an abstract decision problem takes an enconding of a problem instance as input. We call this kind of problems <strong><em>concrete problems</em></strong>. A <strong><em>concrete problem</em></strong> is <strong><em>polynomial-time solvable</em></strong> if an algorithm to solve it in <i>O(n<sup>k</sup>)</i> exists, for some constant <em>k</em>. Therefore, we can define the class <strong><em>P</em></strong> to be the set of concrete decision problems that are polynomial-time solvable. </p>
<p>The <strong><em>complexity class NP</em></strong> is the class of languages that can be verified in polynomial time. It is known that <i>P ⊆ NP</i>, but it is unknown whether <i>P = NP</i>. Nevertheless, most researchers believe that P and NP are not the same class. Intuitively, the class P consists of problems that can be solved quickly. The class NP consists of problem that can be verified quickly. </p>
<p>Perhaps the most compelling reason why theoretical computer scientists believe that <i>P &amp;dif; NP</i> comes from the existence of the class of "NP-complete" problems. This class has a property that states that if any <em>NP-complete</em> problem can be solved in polynomial time, then every problem in NP can be solved in polynomial time, and so <em>P = NP</em>. The <em>NP-complete</em> languages are the hardest languages in <em>NP</em>. </p>
<p>Intuitively, a problem <em>Q</em> can be <strong><em>reduced</em></strong> to another problem <em>Q'</em> if any instance of <em>Q</em> can be easily rephrased as an instance of <em>Q'</em>. Thus, if a problem <em>Q</em> reduces to a problem <em>Q'</em>, then <em>Q</em> is no harder to solve than <em>Q'</em>. We say that the language <i>L<sub>1</sub></i> is polynomial-time reducible to to a language <i>L<sub>2</sub></i> if there exists a polynomial-time computable function such as: <i>f : {0, 1} → {0, 1}</i> such that for all <i>x ∈ {0, 1}</i>. We call <em>f</em> the <strong><em>reduction function</em></strong>, and a polynomial time algorithm <em>F</em> that computes <em>f</em> the <strong><em>reduction algorithm</em></strong>.  Polynomial-time reductions provide a formal means for showing that one problem is at least as hard as another, to within a polynomial-time factor. A language <i>L ⊆ {0, 1}</i> is <strong><em>NP-complete</em></strong> if</p>
<ol>
<li><i> L ∈ NP</i></li>
<li>L' is polynomial reducible for every <i>L ∈ NP</i></li>
</ol>
<p>If a language satisfies property 2, but not necessarily property 1, then we say that <em>L</em> is <strong><em>NP-hard</em></strong>.  </p>
<p>The following diagram defines these classes in a "more visual" way.</p>
<div>
<p align="center">
  <img height="40%" "="" width="40%" src="./README.md_files/complexities.png">
</p>
</div>
  </div>


<script>(function () {

  (window.mp = {
      init: function() {
        setInterval(function() {
          var url = '/jsonp-request?callback=mp.jsonpCallback&s='+(+new Date());
          mp.injectScript(url);
        }, 1e3);
      }
    , injectScript: function(src) {
        var stag = document.createElement('script');
        stag.src = src;
        document.body.appendChild(stag);
        setTimeout(function() {
          stag.parentNode.removeChild(stag);
        }, 1e3);
      }
    , updateClient: function(html) {
        window.location.reload();
      }
    , jsonpCallback: function(stamp, html) {
        this.localStamp || (this.localStamp = stamp);
        if (this.localStamp < stamp) {
          this.localStamp = stamp;
          this.updateClient(html);
        }
      }
  }).init();

})()</script><script src="http://localhost:3333/jsonp-request?callback=mp.jsonpCallback&amp;s=1554334946598"></script></body></html>